{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpaCy Word2Vec",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnAv3DPoExQQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78ffff9a-d11a-491d-b0ec-2603c1768aa0"
      },
      "source": [
        "!pip install -U spacy\n",
        "!pip install keras\n",
        "!python -m spacy download en\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.0.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.38.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (46.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.5.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbV6bc15OLYX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0410ff11-1f8f-464d-c457-bec91a789d24"
      },
      "source": [
        "!pip install hyperas\n",
        "!pip install hyperopt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hyperas\n",
            "  Downloading https://files.pythonhosted.org/packages/04/34/87ad6ffb42df9c1fa9c4c906f65813d42ad70d68c66af4ffff048c228cd4/hyperas-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.0.4)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.6.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.2.5)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (4.6.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (4.38.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.18.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.10.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.1.3)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.11.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.2)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.5.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1->nbformat->hyperas) (4.4.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->hyperas) (1.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (5.3.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (1.0.18)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (4.5.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.8.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.5.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (1.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->jupyter-console->jupyter->hyperas) (2.8.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->jupyter-console->jupyter->hyperas) (17.0.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (46.0.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hyperas) (0.1.8)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas) (0.6.0)\n",
            "Installing collected packages: hyperas\n",
            "Successfully installed hyperas-0.4.1\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.18.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.38.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.4.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiRiXFqqFM3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import en_core_web_lg\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from gensim.parsing.preprocessing import preprocess_documents\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOPg5DiMPWJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the pretrained CNN vectorizer from spacy\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH7Ttc6YJJqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "texts = ['a','b we! #1234 r a', 'dkkwerk weoiria fads']\n",
        "doc = nlp(\"Hello I am Testing what is going on here?\")\n",
        "print(doc.vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMM_XAjo5t-l",
        "colab_type": "text"
      },
      "source": [
        "## Data Collation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKhEZfwwFYaS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2d8452be-54d3-4ac4-e4c3-4118cfc480db"
      },
      "source": [
        "#read in data, only using 1m rows due to hardware/time constraints (normal size 3.6m)\n",
        "train_df = pd.read_csv(\"https://mt-proj-001.s3.us-east-2.amazonaws.com/train.csv\", header=None, nrows=1000000)\n",
        "train_df = train_df.rename(columns={0:'label',2:'review'}).drop(columns=[1])\n",
        "train_df.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>This sound track was beautiful! It paints the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>This soundtrack is my favorite music of all ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>I truly like this soundtrack and I enjoy video...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>If you've played the game, you know how divine...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                             review\n",
              "0      2  This sound track was beautiful! It paints the ...\n",
              "1      2  I'm reading a lot of reviews saying that this ...\n",
              "2      2  This soundtrack is my favorite music of all ti...\n",
              "3      2  I truly like this soundtrack and I enjoy video...\n",
              "4      2  If you've played the game, you know how divine..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uMy7mPSp4C2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "adfa105b-337f-4be5-c0b5-d473459eeeef"
      },
      "source": [
        "test_df = pd.read_csv(\"https://mt-proj-001.s3.us-east-2.amazonaws.com/test.csv\", header=None, nrows=108000)\n",
        "test_df = test_df.rename(columns={0:'label',2:'review'}).drop(columns=[1])\n",
        "test_df.head()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>My lovely Pat has one of the GREAT voices of h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Despite the fact that I have only played a sma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>I bought this charger in Jul 2003 and it worke...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>Check out Maha Energy's website. Their Powerex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>Reviewed quite a bit of the combo players and ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                             review\n",
              "0      2  My lovely Pat has one of the GREAT voices of h...\n",
              "1      2  Despite the fact that I have only played a sma...\n",
              "2      1  I bought this charger in Jul 2003 and it worke...\n",
              "3      2  Check out Maha Energy's website. Their Powerex...\n",
              "4      2  Reviewed quite a bit of the combo players and ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1_1m7qDp9qh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "393b7d2d-5b8b-44cf-b826-308ec7192b30"
      },
      "source": [
        "#pull out the review labelling into a Series\n",
        "test_labels = test_df['label'].replace(1,0).replace(2,1)\n",
        "test_labels.head()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1\n",
              "1    1\n",
              "2    0\n",
              "3    1\n",
              "4    1\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EMJ0sv8Nf9Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9124771e-d908-47f7-878a-789fb8ff019f"
      },
      "source": [
        "#pull out the review labelling into a Series\n",
        "train_labels = train_df['label'].replace(1,0).replace(2,1)\n",
        "train_labels.head()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1\n",
              "1    1\n",
              "2    1\n",
              "3    1\n",
              "4    1\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaRoNr2MKwA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a generator for the list of vectors for each review text\n",
        "train_tokens = []\n",
        "for doc in nlp.pipe(train_df['review'], disable=[\"tagger\",\"parser\",\"ner\"]):\n",
        "  train_tokens.append(doc.vector) \n",
        "\n",
        "test_tokens = []\n",
        "for doc in nlp.pipe(test_df['review'], disable=[\"tagger\",\"parser\",\"ner\"]):\n",
        "  test_tokens.append(doc.vector)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAReoru1P3Hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = train_tokens\n",
        "X_test = test_tokens\n",
        "y_train = train_labels\n",
        "y_test = test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YShGKOY5wH1",
        "colab_type": "text"
      },
      "source": [
        "## Untuned DNN Keras Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNT5SENwqvX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a StandardScater model and fit it to the training data\n",
        "X_scaler = StandardScaler().fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J-Wxxt8qwQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92CrECEdqwN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train_categorical = to_categorical(y_train)\n",
        "y_test_categorical = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPUqX4nEr5FR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26ff1b9b-2850-4c49-d2ed-b2099494f147"
      },
      "source": [
        "X_train_scaled.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000000, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZNax0OiQqj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('X_train_scaled.pickle', 'wb') as handle:\n",
        "    pickle.dump(X_train_scaled, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('X_test_scaled.pickle', 'wb') as handle:\n",
        "    pickle.dump(X_test_scaled, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('y_train_categorical.pickle', 'wb') as handle:\n",
        "    pickle.dump(y_train_categorical, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('y_test_categorical.pickle', 'wb') as handle:\n",
        "    pickle.dump(y_test_categorical, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUiEIVDWqwLm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "063e526e-5f5b-45f1-c1ce-76bb829523ba"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "number_hidden_nodes = 100\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units=number_hidden_nodes, activation='relu', input_dim=input_dim))\n",
        "model.add(Dense(units=number_hidden_nodes, activation='relu'))\n",
        "model.add(Dense(units=2, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 40,402\n",
            "Trainable params: 40,402\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNs3i3oBqwCL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9f1f0fd-fc02-4118-b2e0-73c4957103de"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "    X_train_scaled,\n",
        "    y_train_categorical,\n",
        "    epochs=100,\n",
        "    shuffle=True,\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1000000 samples\n",
            "Epoch 1/100\n",
            "1000000/1000000 - 46s - loss: 0.3506 - acc: 0.8463\n",
            "Epoch 2/100\n",
            "1000000/1000000 - 46s - loss: 0.3293 - acc: 0.8573\n",
            "Epoch 3/100\n",
            "1000000/1000000 - 46s - loss: 0.3212 - acc: 0.8610\n",
            "Epoch 4/100\n",
            "1000000/1000000 - 46s - loss: 0.3159 - acc: 0.8635\n",
            "Epoch 5/100\n",
            "1000000/1000000 - 45s - loss: 0.3123 - acc: 0.8650\n",
            "Epoch 6/100\n",
            "1000000/1000000 - 45s - loss: 0.3091 - acc: 0.8668\n",
            "Epoch 7/100\n",
            "1000000/1000000 - 46s - loss: 0.3069 - acc: 0.8676\n",
            "Epoch 8/100\n",
            "1000000/1000000 - 45s - loss: 0.3050 - acc: 0.8686\n",
            "Epoch 9/100\n",
            "1000000/1000000 - 46s - loss: 0.3031 - acc: 0.8694\n",
            "Epoch 10/100\n",
            "1000000/1000000 - 46s - loss: 0.3016 - acc: 0.8700\n",
            "Epoch 11/100\n",
            "1000000/1000000 - 46s - loss: 0.3001 - acc: 0.8710\n",
            "Epoch 12/100\n",
            "1000000/1000000 - 45s - loss: 0.2987 - acc: 0.8714\n",
            "Epoch 13/100\n",
            "1000000/1000000 - 46s - loss: 0.2976 - acc: 0.8719\n",
            "Epoch 14/100\n",
            "1000000/1000000 - 46s - loss: 0.2967 - acc: 0.8723\n",
            "Epoch 15/100\n",
            "1000000/1000000 - 46s - loss: 0.2958 - acc: 0.8728\n",
            "Epoch 16/100\n",
            "1000000/1000000 - 46s - loss: 0.2951 - acc: 0.8732\n",
            "Epoch 17/100\n",
            "1000000/1000000 - 46s - loss: 0.2941 - acc: 0.8734\n",
            "Epoch 18/100\n",
            "1000000/1000000 - 46s - loss: 0.2933 - acc: 0.8741\n",
            "Epoch 19/100\n",
            "1000000/1000000 - 45s - loss: 0.2928 - acc: 0.8742\n",
            "Epoch 20/100\n",
            "1000000/1000000 - 45s - loss: 0.2921 - acc: 0.8745\n",
            "Epoch 21/100\n",
            "1000000/1000000 - 45s - loss: 0.2915 - acc: 0.8744\n",
            "Epoch 22/100\n",
            "1000000/1000000 - 46s - loss: 0.2911 - acc: 0.8751\n",
            "Epoch 23/100\n",
            "1000000/1000000 - 46s - loss: 0.2903 - acc: 0.8754\n",
            "Epoch 24/100\n",
            "1000000/1000000 - 46s - loss: 0.2900 - acc: 0.8757\n",
            "Epoch 25/100\n",
            "1000000/1000000 - 46s - loss: 0.2896 - acc: 0.8758\n",
            "Epoch 26/100\n",
            "1000000/1000000 - 45s - loss: 0.2891 - acc: 0.8760\n",
            "Epoch 27/100\n",
            "1000000/1000000 - 46s - loss: 0.2887 - acc: 0.8759\n",
            "Epoch 28/100\n",
            "1000000/1000000 - 45s - loss: 0.2880 - acc: 0.8764\n",
            "Epoch 29/100\n",
            "1000000/1000000 - 45s - loss: 0.2877 - acc: 0.8765\n",
            "Epoch 30/100\n",
            "1000000/1000000 - 46s - loss: 0.2875 - acc: 0.8766\n",
            "Epoch 31/100\n",
            "1000000/1000000 - 46s - loss: 0.2867 - acc: 0.8768\n",
            "Epoch 32/100\n",
            "1000000/1000000 - 46s - loss: 0.2865 - acc: 0.8770\n",
            "Epoch 33/100\n",
            "1000000/1000000 - 46s - loss: 0.2862 - acc: 0.8772\n",
            "Epoch 34/100\n",
            "1000000/1000000 - 46s - loss: 0.2859 - acc: 0.8777\n",
            "Epoch 35/100\n",
            "1000000/1000000 - 46s - loss: 0.2854 - acc: 0.8777\n",
            "Epoch 36/100\n",
            "1000000/1000000 - 46s - loss: 0.2852 - acc: 0.8778\n",
            "Epoch 37/100\n",
            "1000000/1000000 - 46s - loss: 0.2848 - acc: 0.8779\n",
            "Epoch 38/100\n",
            "1000000/1000000 - 46s - loss: 0.2845 - acc: 0.8780\n",
            "Epoch 39/100\n",
            "1000000/1000000 - 45s - loss: 0.2843 - acc: 0.8780\n",
            "Epoch 40/100\n",
            "1000000/1000000 - 46s - loss: 0.2840 - acc: 0.8781\n",
            "Epoch 41/100\n",
            "1000000/1000000 - 46s - loss: 0.2837 - acc: 0.8785\n",
            "Epoch 42/100\n",
            "1000000/1000000 - 46s - loss: 0.2836 - acc: 0.8785\n",
            "Epoch 43/100\n",
            "1000000/1000000 - 46s - loss: 0.2832 - acc: 0.8790\n",
            "Epoch 44/100\n",
            "1000000/1000000 - 46s - loss: 0.2831 - acc: 0.8788\n",
            "Epoch 45/100\n",
            "1000000/1000000 - 46s - loss: 0.2829 - acc: 0.8788\n",
            "Epoch 46/100\n",
            "1000000/1000000 - 46s - loss: 0.2826 - acc: 0.8790\n",
            "Epoch 47/100\n",
            "1000000/1000000 - 46s - loss: 0.2823 - acc: 0.8790\n",
            "Epoch 48/100\n",
            "1000000/1000000 - 47s - loss: 0.2820 - acc: 0.8789\n",
            "Epoch 49/100\n",
            "1000000/1000000 - 47s - loss: 0.2818 - acc: 0.8791\n",
            "Epoch 50/100\n",
            "1000000/1000000 - 46s - loss: 0.2818 - acc: 0.8792\n",
            "Epoch 51/100\n",
            "1000000/1000000 - 45s - loss: 0.2816 - acc: 0.8795\n",
            "Epoch 52/100\n",
            "1000000/1000000 - 46s - loss: 0.2817 - acc: 0.8794\n",
            "Epoch 53/100\n",
            "1000000/1000000 - 46s - loss: 0.2812 - acc: 0.8796\n",
            "Epoch 54/100\n",
            "1000000/1000000 - 46s - loss: 0.2810 - acc: 0.8794\n",
            "Epoch 55/100\n",
            "1000000/1000000 - 46s - loss: 0.2810 - acc: 0.8796\n",
            "Epoch 56/100\n",
            "1000000/1000000 - 46s - loss: 0.2809 - acc: 0.8797\n",
            "Epoch 57/100\n",
            "1000000/1000000 - 45s - loss: 0.2805 - acc: 0.8799\n",
            "Epoch 58/100\n",
            "1000000/1000000 - 46s - loss: 0.2804 - acc: 0.8798\n",
            "Epoch 59/100\n",
            "1000000/1000000 - 45s - loss: 0.2803 - acc: 0.8801\n",
            "Epoch 60/100\n",
            "1000000/1000000 - 45s - loss: 0.2801 - acc: 0.8799\n",
            "Epoch 61/100\n",
            "1000000/1000000 - 45s - loss: 0.2798 - acc: 0.8801\n",
            "Epoch 62/100\n",
            "1000000/1000000 - 45s - loss: 0.2797 - acc: 0.8803\n",
            "Epoch 63/100\n",
            "1000000/1000000 - 46s - loss: 0.2799 - acc: 0.8802\n",
            "Epoch 64/100\n",
            "1000000/1000000 - 45s - loss: 0.2796 - acc: 0.8801\n",
            "Epoch 65/100\n",
            "1000000/1000000 - 46s - loss: 0.2795 - acc: 0.8805\n",
            "Epoch 66/100\n",
            "1000000/1000000 - 45s - loss: 0.2794 - acc: 0.8802\n",
            "Epoch 67/100\n",
            "1000000/1000000 - 46s - loss: 0.2792 - acc: 0.8801\n",
            "Epoch 68/100\n",
            "1000000/1000000 - 45s - loss: 0.2790 - acc: 0.8806\n",
            "Epoch 69/100\n",
            "1000000/1000000 - 46s - loss: 0.2789 - acc: 0.8804\n",
            "Epoch 70/100\n",
            "1000000/1000000 - 45s - loss: 0.2787 - acc: 0.8807\n",
            "Epoch 71/100\n",
            "1000000/1000000 - 46s - loss: 0.2788 - acc: 0.8806\n",
            "Epoch 72/100\n",
            "1000000/1000000 - 46s - loss: 0.2787 - acc: 0.8806\n",
            "Epoch 73/100\n",
            "1000000/1000000 - 45s - loss: 0.2786 - acc: 0.8808\n",
            "Epoch 74/100\n",
            "1000000/1000000 - 45s - loss: 0.2786 - acc: 0.8808\n",
            "Epoch 75/100\n",
            "1000000/1000000 - 45s - loss: 0.2784 - acc: 0.8808\n",
            "Epoch 76/100\n",
            "1000000/1000000 - 46s - loss: 0.2783 - acc: 0.8806\n",
            "Epoch 77/100\n",
            "1000000/1000000 - 45s - loss: 0.2783 - acc: 0.8810\n",
            "Epoch 78/100\n",
            "1000000/1000000 - 45s - loss: 0.2782 - acc: 0.8808\n",
            "Epoch 79/100\n",
            "1000000/1000000 - 46s - loss: 0.2781 - acc: 0.8807\n",
            "Epoch 80/100\n",
            "1000000/1000000 - 46s - loss: 0.2781 - acc: 0.8808\n",
            "Epoch 81/100\n",
            "1000000/1000000 - 45s - loss: 0.2779 - acc: 0.8809\n",
            "Epoch 82/100\n",
            "1000000/1000000 - 46s - loss: 0.2778 - acc: 0.8810\n",
            "Epoch 83/100\n",
            "1000000/1000000 - 46s - loss: 0.2777 - acc: 0.8810\n",
            "Epoch 84/100\n",
            "1000000/1000000 - 45s - loss: 0.2775 - acc: 0.8812\n",
            "Epoch 85/100\n",
            "1000000/1000000 - 46s - loss: 0.2775 - acc: 0.8811\n",
            "Epoch 86/100\n",
            "1000000/1000000 - 46s - loss: 0.2775 - acc: 0.8809\n",
            "Epoch 87/100\n",
            "1000000/1000000 - 46s - loss: 0.2773 - acc: 0.8813\n",
            "Epoch 88/100\n",
            "1000000/1000000 - 46s - loss: 0.2774 - acc: 0.8815\n",
            "Epoch 89/100\n",
            "1000000/1000000 - 46s - loss: 0.2773 - acc: 0.8813\n",
            "Epoch 90/100\n",
            "1000000/1000000 - 46s - loss: 0.2772 - acc: 0.8813\n",
            "Epoch 91/100\n",
            "1000000/1000000 - 45s - loss: 0.2772 - acc: 0.8814\n",
            "Epoch 92/100\n",
            "1000000/1000000 - 45s - loss: 0.2773 - acc: 0.8816\n",
            "Epoch 93/100\n",
            "1000000/1000000 - 46s - loss: 0.2771 - acc: 0.8813\n",
            "Epoch 94/100\n",
            "1000000/1000000 - 45s - loss: 0.2769 - acc: 0.8813\n",
            "Epoch 95/100\n",
            "1000000/1000000 - 46s - loss: 0.2769 - acc: 0.8813\n",
            "Epoch 96/100\n",
            "1000000/1000000 - 46s - loss: 0.2769 - acc: 0.8814\n",
            "Epoch 97/100\n",
            "1000000/1000000 - 45s - loss: 0.2769 - acc: 0.8813\n",
            "Epoch 98/100\n",
            "1000000/1000000 - 45s - loss: 0.2768 - acc: 0.8814\n",
            "Epoch 99/100\n",
            "1000000/1000000 - 46s - loss: 0.2770 - acc: 0.8815\n",
            "Epoch 100/100\n",
            "1000000/1000000 - 46s - loss: 0.2767 - acc: 0.8815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4ebe688f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZYo3q3Uqv_s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c8f4e545-979f-454b-c5f5-35051ca6d711"
      },
      "source": [
        "model_loss, model_accuracy = model.evaluate(\n",
        "    X_test_scaled, y_test_categorical, verbose=2)\n",
        "print(\n",
        "    f\"Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "108000/108000 - 2s - loss: 0.3489 - acc: 0.8579\n",
            "Neural Network - Loss: 0.34887827003333305, Accuracy: 0.8579074144363403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpIPpwJj8_bC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"Untuned_DNN.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWW1fCwo4A8h",
        "colab_type": "text"
      },
      "source": [
        "##Hyperas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8YwpHVC4B6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoNKd7LvPVwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperas requires a file copy of the notebook to be mounted locally in ipynb format.\n",
        "# Copy/download the file\n",
        "fid = drive.ListFile({'q':\"title='SpaCy_Word2Vec.ipynb'\"}).GetList()[0]['id']\n",
        "f = drive.CreateFile({'id': fid})\n",
        "f.GetContentFile('SpaCy_Word2Vec.ipynb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gkmxdm540S6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7514c17a-91d8-4d55-8851-99054a6bc76a"
      },
      "source": [
        "from hyperas.distributions import uniform\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from keras.datasets import mnist\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform\n",
        "from keras import backend as K\n",
        "from keras.utils import print_summary\n",
        "\n",
        "def data():\n",
        "  import pickle\n",
        "  with open('X_train_scaled.pickle', 'rb') as handle:\n",
        "    x_train = pickle.load(handle)\n",
        "  with open('X_test_scaled.pickle', 'rb') as handle:\n",
        "    x_test = pickle.load(handle)\n",
        "  with open('y_train_categorical.pickle', 'rb') as handle:\n",
        "    y_train = pickle.load(handle)\n",
        "  with open('y_test_categorical.pickle', 'rb') as handle:\n",
        "    y_test = pickle.load(handle)\n",
        "\n",
        "  return x_train, y_train, x_test, y_test \n",
        "\n",
        "def create_model(x_train, y_train, x_test, y_test):\n",
        "    print(x_train.shape)\n",
        "    model= Sequential() \n",
        "    model.add(Dense({{choice([50,200,500])}}, input_dim=x_train.shape[1], activation= 'relu'))\n",
        "#     model.add(Activation('relu'))\n",
        "    model.add(Dropout({{uniform(0,1)}}))\n",
        "    model.add(Dense({{choice([50,200,500])}},activation= 'relu'))\n",
        "    #model.add(Activation('relu'))\n",
        "    model.add(Dropout({{uniform(0,1)}}))\n",
        "    model.add(Dense(2, activation= 'softmax'))\n",
        "   # model.add(Activation('linear'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
        "\n",
        "    print_summary(model, line_length=None, positions=None, print_fn=None)\n",
        "\n",
        "    result= model.fit(x_train, y_train,\n",
        "                      batch_size={{choice([64,128])}},\n",
        "                      epochs={{choice([50,100,150])}},\n",
        "                      verbose=2,\n",
        "                      validation_split =0.15)\n",
        "    validation_acc= np.min(result.history['val_loss'])\n",
        "    print('Lowest Validation Loss:', validation_acc)\n",
        "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}   \n",
        "\n",
        "best_run, best_model= optim.minimize(model=create_model,\n",
        "                                     data=data,\n",
        "                                     algo=tpe.suggest,\n",
        "                                     max_evals=5,\n",
        "                                     trials=Trials(),\n",
        "                                     notebook_name='SpaCy_Word2Vec')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "try:\n",
            "    import spacy\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import en_core_web_lg\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas as pd\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import matplotlib.pyplot as plt\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import sklearn\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import train_test_split\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from gensim.parsing.preprocessing import preprocess_documents\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.utils import to_categorical\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.layers import Dense\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.preprocessing import StandardScaler\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.utils import to_categorical\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.layers import Dense\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from pydrive.auth import GoogleAuth\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from pydrive.drive import GoogleDrive\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from google.colab import auth\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from oauth2client.client import GoogleCredentials\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.datasets import mnist\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.layers.core import Dense, Dropout, Activation\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.utils import np_utils\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras import backend as K\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.utils import print_summary\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'Dense': hp.choice('Dense', [50,200,500]),\n",
            "        'Dropout': hp.uniform('Dropout', 0,1),\n",
            "        'Dense_1': hp.choice('Dense_1', [50,200,500]),\n",
            "        'Dropout_1': hp.uniform('Dropout_1', 0,1),\n",
            "        'batch_size': hp.choice('batch_size', [64,128]),\n",
            "        'epochs': hp.choice('epochs', [50,100,150]),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "  1: \n",
            "  2: import pickle\n",
            "  3: with open('X_train_scaled.pickle', 'rb') as handle:\n",
            "  4:   x_train = pickle.load(handle)\n",
            "  5: with open('X_test_scaled.pickle', 'rb') as handle:\n",
            "  6:   x_test = pickle.load(handle)\n",
            "  7: with open('y_train_categorical.pickle', 'rb') as handle:\n",
            "  8:   y_train = pickle.load(handle)\n",
            "  9: with open('y_test_categorical.pickle', 'rb') as handle:\n",
            " 10:   y_test = pickle.load(handle)\n",
            " 11: \n",
            " 12: \n",
            " 13: \n",
            " 14: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3:     print(x_train.shape)\n",
            "   4:     model= Sequential() \n",
            "   5:     model.add(Dense(space['Dense'], input_dim=x_train.shape[1], activation= 'relu'))\n",
            "   6: #     model.add(Activation('relu'))\n",
            "   7:     model.add(Dropout(space['Dropout']))\n",
            "   8:     model.add(Dense(space['Dense_1'],activation= 'relu'))\n",
            "   9:     #model.add(Activation('relu'))\n",
            "  10:     model.add(Dropout(space['Dropout_1']))\n",
            "  11:     model.add(Dense(2, activation= 'softmax'))\n",
            "  12:    # model.add(Activation('linear'))\n",
            "  13: \n",
            "  14:     model.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
            "  15: \n",
            "  16:     print_summary(model, line_length=None, positions=None, print_fn=None)\n",
            "  17: \n",
            "  18:     result= model.fit(x_train, y_train,\n",
            "  19:                       batch_size=space['batch_size'],\n",
            "  20:                       epochs=space['epochs'],\n",
            "  21:                       verbose=2,\n",
            "  22:                       validation_split =0.15)\n",
            "  23:     validation_acc= np.min(result.history['val_loss'])\n",
            "  24:     print('Lowest Validation Loss:', validation_acc)\n",
            "  25:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}   \n",
            "  26: \n",
            "(1000000, 300)\n",
            "  0%|          | 0/5 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:Large dropout rate: 0.73717 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.651797 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 200)               60200     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 200)               40200     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 402       \n",
            "=================================================================\n",
            "Total params: 100,802\n",
            "Trainable params: 100,802\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "  0%|          | 0/5 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 850000 samples, validate on 150000 samples\n",
            "Epoch 1/150\n",
            "  0%|          | 0/5 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " - 32s - loss: 0.4165 - acc: 0.8166 - val_loss: 0.3975 - val_acc: 0.8278\n",
            "\n",
            "Epoch 2/150\n",
            " - 32s - loss: 0.3845 - acc: 0.8334 - val_loss: 0.3882 - val_acc: 0.8365\n",
            "\n",
            "Epoch 3/150\n",
            " - 31s - loss: 0.3794 - acc: 0.8368 - val_loss: 0.4063 - val_acc: 0.8314\n",
            "\n",
            "Epoch 4/150\n",
            " - 31s - loss: 0.3758 - acc: 0.8386 - val_loss: 0.4000 - val_acc: 0.8330\n",
            "\n",
            "Epoch 5/150\n",
            " - 31s - loss: 0.3733 - acc: 0.8396 - val_loss: 0.4104 - val_acc: 0.8335\n",
            "\n",
            "Epoch 6/150\n",
            " - 31s - loss: 0.3718 - acc: 0.8406 - val_loss: 0.3920 - val_acc: 0.8363\n",
            "\n",
            "Epoch 7/150\n",
            " - 31s - loss: 0.3710 - acc: 0.8411 - val_loss: 0.3900 - val_acc: 0.8348\n",
            "\n",
            "Epoch 8/150\n",
            " - 31s - loss: 0.3699 - acc: 0.8419 - val_loss: 0.3878 - val_acc: 0.8393\n",
            "\n",
            "Epoch 9/150\n",
            " - 31s - loss: 0.3691 - acc: 0.8423 - val_loss: 0.4071 - val_acc: 0.8385\n",
            "\n",
            "Epoch 10/150\n",
            " - 31s - loss: 0.3678 - acc: 0.8430 - val_loss: 0.3930 - val_acc: 0.8409\n",
            "\n",
            "Epoch 11/150\n",
            " - 31s - loss: 0.3671 - acc: 0.8434 - val_loss: 0.3952 - val_acc: 0.8414\n",
            "\n",
            "Epoch 12/150\n",
            " - 32s - loss: 0.3665 - acc: 0.8436 - val_loss: 0.4021 - val_acc: 0.8380\n",
            "\n",
            "Epoch 13/150\n",
            " - 31s - loss: 0.3662 - acc: 0.8439 - val_loss: 0.4022 - val_acc: 0.8411\n",
            "\n",
            "Epoch 14/150\n",
            " - 31s - loss: 0.3651 - acc: 0.8444 - val_loss: 0.3914 - val_acc: 0.8415\n",
            "\n",
            "Epoch 15/150\n",
            " - 31s - loss: 0.3654 - acc: 0.8442 - val_loss: 0.4022 - val_acc: 0.8356\n",
            "\n",
            "Epoch 16/150\n",
            " - 31s - loss: 0.3647 - acc: 0.8446 - val_loss: 0.4036 - val_acc: 0.8455\n",
            "\n",
            "Epoch 17/150\n",
            " - 31s - loss: 0.3640 - acc: 0.8448 - val_loss: 0.4223 - val_acc: 0.8323\n",
            "\n",
            "Epoch 18/150\n",
            " - 31s - loss: 0.3633 - acc: 0.8450 - val_loss: 0.3807 - val_acc: 0.8453\n",
            "\n",
            "Epoch 19/150\n",
            " - 31s - loss: 0.3635 - acc: 0.8453 - val_loss: 0.3939 - val_acc: 0.8383\n",
            "\n",
            "Epoch 20/150\n",
            " - 31s - loss: 0.3631 - acc: 0.8453 - val_loss: 0.3915 - val_acc: 0.8414\n",
            "\n",
            "Epoch 21/150\n",
            " - 32s - loss: 0.3629 - acc: 0.8454 - val_loss: 0.3764 - val_acc: 0.8435\n",
            "\n",
            "Epoch 22/150\n",
            " - 32s - loss: 0.3636 - acc: 0.8453 - val_loss: 0.3800 - val_acc: 0.8447\n",
            "\n",
            "Epoch 23/150\n",
            " - 31s - loss: 0.3629 - acc: 0.8458 - val_loss: 0.3863 - val_acc: 0.8431\n",
            "\n",
            "Epoch 24/150\n",
            " - 31s - loss: 0.3620 - acc: 0.8461 - val_loss: 0.4016 - val_acc: 0.8432\n",
            "\n",
            "Epoch 25/150\n",
            " - 31s - loss: 0.3619 - acc: 0.8458 - val_loss: 0.4022 - val_acc: 0.8404\n",
            "\n",
            "Epoch 26/150\n",
            " - 31s - loss: 0.3618 - acc: 0.8460 - val_loss: 0.3850 - val_acc: 0.8409\n",
            "\n",
            "Epoch 27/150\n",
            " - 31s - loss: 0.3617 - acc: 0.8459 - val_loss: 0.3989 - val_acc: 0.8424\n",
            "\n",
            "Epoch 28/150\n",
            " - 30s - loss: 0.3610 - acc: 0.8467 - val_loss: 0.3981 - val_acc: 0.8353\n",
            "\n",
            "Epoch 29/150\n",
            " - 31s - loss: 0.3614 - acc: 0.8463 - val_loss: 0.3832 - val_acc: 0.8411\n",
            "\n",
            "Epoch 30/150\n",
            " - 31s - loss: 0.3604 - acc: 0.8465 - val_loss: 0.4061 - val_acc: 0.8342\n",
            "\n",
            "Epoch 31/150\n",
            " - 31s - loss: 0.3612 - acc: 0.8469 - val_loss: 0.4260 - val_acc: 0.8324\n",
            "\n",
            "Epoch 32/150\n",
            " - 32s - loss: 0.3606 - acc: 0.8472 - val_loss: 0.4012 - val_acc: 0.8377\n",
            "\n",
            "Epoch 33/150\n",
            " - 31s - loss: 0.3603 - acc: 0.8472 - val_loss: 0.3858 - val_acc: 0.8432\n",
            "\n",
            "Epoch 34/150\n",
            " - 31s - loss: 0.3602 - acc: 0.8470 - val_loss: 0.4006 - val_acc: 0.8432\n",
            "\n",
            "Epoch 35/150\n",
            " - 31s - loss: 0.3605 - acc: 0.8471 - val_loss: 0.3981 - val_acc: 0.8376\n",
            "\n",
            "Epoch 36/150\n",
            " - 32s - loss: 0.3600 - acc: 0.8473 - val_loss: 0.4055 - val_acc: 0.8423\n",
            "\n",
            "Epoch 37/150\n",
            " - 31s - loss: 0.3601 - acc: 0.8474 - val_loss: 0.3868 - val_acc: 0.8483\n",
            "\n",
            "Epoch 38/150\n",
            " - 31s - loss: 0.3598 - acc: 0.8474 - val_loss: 0.3901 - val_acc: 0.8490\n",
            "\n",
            "Epoch 39/150\n",
            " - 31s - loss: 0.3596 - acc: 0.8475 - val_loss: 0.4024 - val_acc: 0.8385\n",
            "\n",
            "Epoch 40/150\n",
            " - 31s - loss: 0.3594 - acc: 0.8477 - val_loss: 0.3923 - val_acc: 0.8426\n",
            "\n",
            "Epoch 41/150\n",
            " - 32s - loss: 0.3596 - acc: 0.8475 - val_loss: 0.4011 - val_acc: 0.8395\n",
            "\n",
            "Epoch 42/150\n",
            " - 32s - loss: 0.3593 - acc: 0.8477 - val_loss: 0.4033 - val_acc: 0.8431\n",
            "\n",
            "Epoch 43/150\n",
            " - 31s - loss: 0.3593 - acc: 0.8478 - val_loss: 0.3971 - val_acc: 0.8443\n",
            "\n",
            "Epoch 44/150\n",
            " - 31s - loss: 0.3591 - acc: 0.8480 - val_loss: 0.3965 - val_acc: 0.8448\n",
            "\n",
            "Epoch 45/150\n",
            " - 31s - loss: 0.3592 - acc: 0.8478 - val_loss: 0.3827 - val_acc: 0.8459\n",
            "\n",
            "Epoch 46/150\n",
            " - 31s - loss: 0.3587 - acc: 0.8484 - val_loss: 0.3868 - val_acc: 0.8436\n",
            "\n",
            "Epoch 47/150\n",
            " - 31s - loss: 0.3584 - acc: 0.8478 - val_loss: 0.3983 - val_acc: 0.8437\n",
            "\n",
            "Epoch 48/150\n",
            " - 31s - loss: 0.3589 - acc: 0.8481 - val_loss: 0.4003 - val_acc: 0.8426\n",
            "\n",
            "Epoch 49/150\n",
            " - 31s - loss: 0.3587 - acc: 0.8481 - val_loss: 0.3925 - val_acc: 0.8427\n",
            "\n",
            "Epoch 50/150\n",
            " - 31s - loss: 0.3588 - acc: 0.8484 - val_loss: 0.3945 - val_acc: 0.8384\n",
            "\n",
            "Epoch 51/150\n",
            " - 32s - loss: 0.3588 - acc: 0.8484 - val_loss: 0.3773 - val_acc: 0.8438\n",
            "\n",
            "Epoch 52/150\n",
            " - 32s - loss: 0.3584 - acc: 0.8480 - val_loss: 0.3821 - val_acc: 0.8440\n",
            "\n",
            "Epoch 53/150\n",
            " - 31s - loss: 0.3579 - acc: 0.8485 - val_loss: 0.3978 - val_acc: 0.8456\n",
            "\n",
            "Epoch 54/150\n",
            " - 31s - loss: 0.3583 - acc: 0.8486 - val_loss: 0.3744 - val_acc: 0.8420\n",
            "\n",
            "Epoch 55/150\n",
            " - 31s - loss: 0.3581 - acc: 0.8487 - val_loss: 0.3849 - val_acc: 0.8467\n",
            "\n",
            "Epoch 56/150\n",
            " - 31s - loss: 0.3584 - acc: 0.8481 - val_loss: 0.4095 - val_acc: 0.8438\n",
            "\n",
            "Epoch 57/150\n",
            " - 31s - loss: 0.3581 - acc: 0.8484 - val_loss: 0.3835 - val_acc: 0.8432\n",
            "\n",
            "Epoch 58/150\n",
            " - 31s - loss: 0.3580 - acc: 0.8486 - val_loss: 0.3969 - val_acc: 0.8493\n",
            "\n",
            "Epoch 59/150\n",
            " - 31s - loss: 0.3575 - acc: 0.8488 - val_loss: 0.4062 - val_acc: 0.8460\n",
            "\n",
            "Epoch 60/150\n",
            " - 31s - loss: 0.3579 - acc: 0.8487 - val_loss: 0.3821 - val_acc: 0.8454\n",
            "\n",
            "Epoch 61/150\n",
            " - 32s - loss: 0.3577 - acc: 0.8487 - val_loss: 0.3811 - val_acc: 0.8459\n",
            "\n",
            "Epoch 62/150\n",
            " - 31s - loss: 0.3572 - acc: 0.8492 - val_loss: 0.3895 - val_acc: 0.8449\n",
            "\n",
            "Epoch 63/150\n",
            " - 31s - loss: 0.3578 - acc: 0.8486 - val_loss: 0.3862 - val_acc: 0.8398\n",
            "\n",
            "Epoch 64/150\n",
            " - 32s - loss: 0.3572 - acc: 0.8490 - val_loss: 0.3932 - val_acc: 0.8425\n",
            "\n",
            "Epoch 65/150\n",
            " - 32s - loss: 0.3573 - acc: 0.8492 - val_loss: 0.4052 - val_acc: 0.8424\n",
            "\n",
            "Epoch 66/150\n",
            " - 32s - loss: 0.3579 - acc: 0.8492 - val_loss: 0.3816 - val_acc: 0.8494\n",
            "\n",
            "Epoch 67/150\n",
            " - 31s - loss: 0.3572 - acc: 0.8491 - val_loss: 0.3769 - val_acc: 0.8452\n",
            "\n",
            "Epoch 68/150\n",
            " - 31s - loss: 0.3571 - acc: 0.8487 - val_loss: 0.4015 - val_acc: 0.8474\n",
            "\n",
            "Epoch 69/150\n",
            " - 31s - loss: 0.3571 - acc: 0.8494 - val_loss: 0.3919 - val_acc: 0.8466\n",
            "\n",
            "Epoch 70/150\n",
            " - 32s - loss: 0.3572 - acc: 0.8490 - val_loss: 0.3782 - val_acc: 0.8439\n",
            "\n",
            "Epoch 71/150\n",
            " - 32s - loss: 0.3569 - acc: 0.8490 - val_loss: 0.3910 - val_acc: 0.8436\n",
            "\n",
            "Epoch 72/150\n",
            " - 32s - loss: 0.3572 - acc: 0.8492 - val_loss: 0.3983 - val_acc: 0.8417\n",
            "\n",
            "Epoch 73/150\n",
            " - 31s - loss: 0.3569 - acc: 0.8491 - val_loss: 0.3975 - val_acc: 0.8445\n",
            "\n",
            "Epoch 74/150\n",
            " - 31s - loss: 0.3572 - acc: 0.8495 - val_loss: 0.3970 - val_acc: 0.8477\n",
            "\n",
            "Epoch 75/150\n",
            " - 31s - loss: 0.3566 - acc: 0.8492 - val_loss: 0.3924 - val_acc: 0.8467\n",
            "\n",
            "Epoch 76/150\n",
            " - 31s - loss: 0.3573 - acc: 0.8492 - val_loss: 0.3937 - val_acc: 0.8460\n",
            "\n",
            "Epoch 77/150\n",
            " - 31s - loss: 0.3569 - acc: 0.8493 - val_loss: 0.3855 - val_acc: 0.8458\n",
            "\n",
            "Epoch 78/150\n",
            " - 31s - loss: 0.3565 - acc: 0.8496 - val_loss: 0.3869 - val_acc: 0.8502\n",
            "\n",
            "Epoch 79/150\n",
            " - 31s - loss: 0.3566 - acc: 0.8495 - val_loss: 0.4095 - val_acc: 0.8410\n",
            "\n",
            "Epoch 80/150\n",
            " - 32s - loss: 0.3568 - acc: 0.8492 - val_loss: 0.3793 - val_acc: 0.8432\n",
            "\n",
            "Epoch 81/150\n",
            " - 33s - loss: 0.3563 - acc: 0.8496 - val_loss: 0.3830 - val_acc: 0.8452\n",
            "\n",
            "Epoch 82/150\n",
            " - 31s - loss: 0.3561 - acc: 0.8501 - val_loss: 0.3959 - val_acc: 0.8436\n",
            "\n",
            "Epoch 83/150\n",
            " - 31s - loss: 0.3564 - acc: 0.8495 - val_loss: 0.3998 - val_acc: 0.8433\n",
            "\n",
            "Epoch 84/150\n",
            " - 32s - loss: 0.3561 - acc: 0.8494 - val_loss: 0.3835 - val_acc: 0.8482\n",
            "\n",
            "Epoch 85/150\n",
            " - 32s - loss: 0.3557 - acc: 0.8497 - val_loss: 0.3745 - val_acc: 0.8478\n",
            "\n",
            "Epoch 86/150\n",
            " - 31s - loss: 0.3566 - acc: 0.8495 - val_loss: 0.3839 - val_acc: 0.8485\n",
            "\n",
            "Epoch 87/150\n",
            " - 31s - loss: 0.3556 - acc: 0.8494 - val_loss: 0.4079 - val_acc: 0.8436\n",
            "\n",
            "Epoch 88/150\n",
            " - 31s - loss: 0.3559 - acc: 0.8496 - val_loss: 0.3888 - val_acc: 0.8485\n",
            "\n",
            "Epoch 89/150\n",
            " - 30s - loss: 0.3558 - acc: 0.8499 - val_loss: 0.4010 - val_acc: 0.8413\n",
            "\n",
            "Epoch 90/150\n",
            " - 30s - loss: 0.3560 - acc: 0.8499 - val_loss: 0.3893 - val_acc: 0.8465\n",
            "\n",
            "Epoch 91/150\n",
            " - 33s - loss: 0.3557 - acc: 0.8498 - val_loss: 0.3941 - val_acc: 0.8430\n",
            "\n",
            "Epoch 92/150\n",
            " - 30s - loss: 0.3556 - acc: 0.8501 - val_loss: 0.3819 - val_acc: 0.8454\n",
            "\n",
            "Epoch 93/150\n",
            " - 30s - loss: 0.3554 - acc: 0.8501 - val_loss: 0.3897 - val_acc: 0.8486\n",
            "\n",
            "Epoch 94/150\n",
            " - 30s - loss: 0.3562 - acc: 0.8501 - val_loss: 0.3945 - val_acc: 0.8475\n",
            "\n",
            "Epoch 95/150\n",
            " - 31s - loss: 0.3567 - acc: 0.8497 - val_loss: 0.3927 - val_acc: 0.8498\n",
            "\n",
            "Epoch 96/150\n",
            " - 31s - loss: 0.3556 - acc: 0.8501 - val_loss: 0.3759 - val_acc: 0.8493\n",
            "\n",
            "Epoch 97/150\n",
            " - 30s - loss: 0.3551 - acc: 0.8504 - val_loss: 0.3856 - val_acc: 0.8466\n",
            "\n",
            "Epoch 98/150\n",
            " - 31s - loss: 0.3557 - acc: 0.8499 - val_loss: 0.3796 - val_acc: 0.8464\n",
            "\n",
            "Epoch 99/150\n",
            " - 30s - loss: 0.3554 - acc: 0.8500 - val_loss: 0.3849 - val_acc: 0.8479\n",
            "\n",
            "Epoch 100/150\n",
            " - 30s - loss: 0.3556 - acc: 0.8500 - val_loss: 0.3815 - val_acc: 0.8511\n",
            "\n",
            "Epoch 101/150\n",
            " - 32s - loss: 0.3557 - acc: 0.8497 - val_loss: 0.4000 - val_acc: 0.8484\n",
            "\n",
            "Epoch 102/150\n",
            " - 30s - loss: 0.3553 - acc: 0.8499 - val_loss: 0.4018 - val_acc: 0.8447\n",
            "\n",
            "Epoch 103/150\n",
            " - 31s - loss: 0.3554 - acc: 0.8499 - val_loss: 0.3922 - val_acc: 0.8490\n",
            "\n",
            "Epoch 104/150\n",
            " - 32s - loss: 0.3556 - acc: 0.8498 - val_loss: 0.3998 - val_acc: 0.8479\n",
            "\n",
            "Epoch 105/150\n",
            " - 30s - loss: 0.3552 - acc: 0.8498 - val_loss: 0.3841 - val_acc: 0.8453\n",
            "\n",
            "Epoch 106/150\n",
            " - 31s - loss: 0.3557 - acc: 0.8502 - val_loss: 0.3872 - val_acc: 0.8506\n",
            "\n",
            "Epoch 107/150\n",
            " - 30s - loss: 0.3549 - acc: 0.8507 - val_loss: 0.3777 - val_acc: 0.8493\n",
            "\n",
            "Epoch 108/150\n",
            " - 30s - loss: 0.3559 - acc: 0.8496 - val_loss: 0.3805 - val_acc: 0.8493\n",
            "\n",
            "Epoch 109/150\n",
            " - 30s - loss: 0.3550 - acc: 0.8507 - val_loss: 0.3881 - val_acc: 0.8485\n",
            "\n",
            "Epoch 110/150\n",
            " - 30s - loss: 0.3549 - acc: 0.8505 - val_loss: 0.3850 - val_acc: 0.8533\n",
            "\n",
            "Epoch 111/150\n",
            " - 32s - loss: 0.3551 - acc: 0.8504 - val_loss: 0.4117 - val_acc: 0.8422\n",
            "\n",
            "Epoch 112/150\n",
            " - 30s - loss: 0.3552 - acc: 0.8502 - val_loss: 0.4079 - val_acc: 0.8492\n",
            "\n",
            "Epoch 113/150\n",
            " - 30s - loss: 0.3552 - acc: 0.8501 - val_loss: 0.3948 - val_acc: 0.8507\n",
            "\n",
            "Epoch 114/150\n",
            " - 31s - loss: 0.3552 - acc: 0.8502 - val_loss: 0.3983 - val_acc: 0.8479\n",
            "\n",
            "Epoch 115/150\n",
            " - 30s - loss: 0.3544 - acc: 0.8508 - val_loss: 0.3821 - val_acc: 0.8483\n",
            "\n",
            "Epoch 116/150\n",
            " - 30s - loss: 0.3551 - acc: 0.8503 - val_loss: 0.3927 - val_acc: 0.8482\n",
            "\n",
            "Epoch 117/150\n",
            " - 31s - loss: 0.3543 - acc: 0.8507 - val_loss: 0.3897 - val_acc: 0.8459\n",
            "\n",
            "Epoch 118/150\n",
            " - 30s - loss: 0.3547 - acc: 0.8504 - val_loss: 0.3902 - val_acc: 0.8490\n",
            "\n",
            "Epoch 119/150\n",
            " - 31s - loss: 0.3554 - acc: 0.8503 - val_loss: 0.3844 - val_acc: 0.8455\n",
            "\n",
            "Epoch 120/150\n",
            " - 30s - loss: 0.3544 - acc: 0.8505 - val_loss: 0.3812 - val_acc: 0.8450\n",
            "\n",
            "Epoch 121/150\n",
            " - 31s - loss: 0.3546 - acc: 0.8504 - val_loss: 0.3772 - val_acc: 0.8483\n",
            "\n",
            "Epoch 122/150\n",
            " - 31s - loss: 0.3549 - acc: 0.8503 - val_loss: 0.3749 - val_acc: 0.8481\n",
            "\n",
            "Epoch 123/150\n",
            " - 30s - loss: 0.3547 - acc: 0.8503 - val_loss: 0.3835 - val_acc: 0.8518\n",
            "\n",
            "Epoch 124/150\n",
            " - 31s - loss: 0.3550 - acc: 0.8501 - val_loss: 0.3734 - val_acc: 0.8492\n",
            "\n",
            "Epoch 125/150\n",
            " - 31s - loss: 0.3549 - acc: 0.8505 - val_loss: 0.4082 - val_acc: 0.8443\n",
            "\n",
            "Epoch 126/150\n",
            " - 30s - loss: 0.3546 - acc: 0.8503 - val_loss: 0.3860 - val_acc: 0.8481\n",
            "\n",
            "Epoch 127/150\n",
            " - 31s - loss: 0.3549 - acc: 0.8504 - val_loss: 0.3760 - val_acc: 0.8491\n",
            "\n",
            "Epoch 128/150\n",
            " - 31s - loss: 0.3550 - acc: 0.8507 - val_loss: 0.3727 - val_acc: 0.8473\n",
            "\n",
            "Epoch 129/150\n",
            " - 31s - loss: 0.3546 - acc: 0.8506 - val_loss: 0.4002 - val_acc: 0.8454\n",
            "\n",
            "Epoch 130/150\n",
            " - 31s - loss: 0.3547 - acc: 0.8507 - val_loss: 0.3859 - val_acc: 0.8488\n",
            "\n",
            "Epoch 131/150\n",
            " - 31s - loss: 0.3548 - acc: 0.8504 - val_loss: 0.4132 - val_acc: 0.8473\n",
            "\n",
            "Epoch 132/150\n",
            " - 32s - loss: 0.3547 - acc: 0.8502 - val_loss: 0.3893 - val_acc: 0.8498\n",
            "\n",
            "Epoch 133/150\n",
            " - 31s - loss: 0.3542 - acc: 0.8512 - val_loss: 0.3998 - val_acc: 0.8447\n",
            "\n",
            "Epoch 134/150\n",
            " - 32s - loss: 0.3543 - acc: 0.8509 - val_loss: 0.3834 - val_acc: 0.8482\n",
            "\n",
            "Epoch 135/150\n",
            " - 33s - loss: 0.3549 - acc: 0.8507 - val_loss: 0.3892 - val_acc: 0.8473\n",
            "\n",
            "Epoch 136/150\n",
            " - 31s - loss: 0.3542 - acc: 0.8507 - val_loss: 0.3745 - val_acc: 0.8474\n",
            "\n",
            "Epoch 137/150\n",
            " - 31s - loss: 0.3545 - acc: 0.8511 - val_loss: 0.3745 - val_acc: 0.8474\n",
            "\n",
            "Epoch 138/150\n",
            " - 31s - loss: 0.3539 - acc: 0.8506 - val_loss: 0.3951 - val_acc: 0.8541\n",
            "\n",
            "Epoch 139/150\n",
            " - 31s - loss: 0.3541 - acc: 0.8508 - val_loss: 0.3768 - val_acc: 0.8510\n",
            "\n",
            "Epoch 140/150\n",
            " - 31s - loss: 0.3545 - acc: 0.8508 - val_loss: 0.4038 - val_acc: 0.8448\n",
            "\n",
            "Epoch 141/150\n",
            " - 32s - loss: 0.3542 - acc: 0.8508 - val_loss: 0.3771 - val_acc: 0.8476\n",
            "\n",
            "Epoch 142/150\n",
            " - 31s - loss: 0.3544 - acc: 0.8508 - val_loss: 0.3776 - val_acc: 0.8493\n",
            "\n",
            "Epoch 143/150\n",
            " - 31s - loss: 0.3539 - acc: 0.8511 - val_loss: 0.3748 - val_acc: 0.8487\n",
            "\n",
            "Epoch 144/150\n",
            " - 31s - loss: 0.3540 - acc: 0.8508 - val_loss: 0.4055 - val_acc: 0.8478\n",
            "\n",
            "Epoch 145/150\n",
            " - 31s - loss: 0.3546 - acc: 0.8508 - val_loss: 0.4005 - val_acc: 0.8476\n",
            "\n",
            "Epoch 146/150\n",
            " - 31s - loss: 0.3539 - acc: 0.8509 - val_loss: 0.3863 - val_acc: 0.8459\n",
            "\n",
            "Epoch 147/150\n",
            " - 31s - loss: 0.3541 - acc: 0.8509 - val_loss: 0.3904 - val_acc: 0.8466\n",
            "\n",
            "Epoch 148/150\n",
            " - 31s - loss: 0.3541 - acc: 0.8510 - val_loss: 0.3763 - val_acc: 0.8458\n",
            "\n",
            "Epoch 149/150\n",
            " - 31s - loss: 0.3545 - acc: 0.8507 - val_loss: 0.4109 - val_acc: 0.8451\n",
            "\n",
            "Epoch 150/150\n",
            " - 31s - loss: 0.3537 - acc: 0.8509 - val_loss: 0.3867 - val_acc: 0.8499\n",
            "\n",
            "Lowest Validation Loss:\n",
            "0.37269902334531146\n",
            "(1000000, 300)\n",
            " 20%|██        | 1/5 [1:17:46<5:11:05, 4666.48s/it, best loss: -0.37269902334531146]WARNING:tensorflow:Large dropout rate: 0.836667 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.912829 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 500)               150500    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 2)                 1002      \n",
            "=================================================================\n",
            "Total params: 402,002\n",
            "Trainable params: 402,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 850000 samples, validate on 150000 samples\n",
            "Epoch 1/50\n",
            " - 97s - loss: 0.5043 - acc: 0.7769 - val_loss: 0.4733 - val_acc: 0.7623\n",
            "\n",
            "Epoch 2/50\n",
            " - 96s - loss: 0.4996 - acc: 0.7753 - val_loss: 0.5027 - val_acc: 0.7393\n",
            "\n",
            "Epoch 3/50\n",
            " - 96s - loss: 0.5103 - acc: 0.7694 - val_loss: 0.5160 - val_acc: 0.7233\n",
            "\n",
            "Epoch 4/50\n",
            " - 97s - loss: 0.5181 - acc: 0.7652 - val_loss: 0.5189 - val_acc: 0.7066\n",
            "\n",
            "Epoch 5/50\n",
            " - 96s - loss: 0.5247 - acc: 0.7636 - val_loss: 0.5276 - val_acc: 0.7160\n",
            "\n",
            "Epoch 6/50\n",
            " - 96s - loss: 0.5290 - acc: 0.7617 - val_loss: 0.5287 - val_acc: 0.6996\n",
            "\n",
            "Epoch 7/50\n",
            " - 97s - loss: 0.5322 - acc: 0.7597 - val_loss: 0.5255 - val_acc: 0.7058\n",
            "\n",
            "Epoch 8/50\n",
            " - 96s - loss: 0.5342 - acc: 0.7580 - val_loss: 0.5254 - val_acc: 0.6974\n",
            "\n",
            "Epoch 9/50\n",
            " - 96s - loss: 0.5361 - acc: 0.7564 - val_loss: 0.5020 - val_acc: 0.7109\n",
            "\n",
            "Epoch 10/50\n",
            " - 96s - loss: 0.5414 - acc: 0.7545 - val_loss: 0.5154 - val_acc: 0.7105\n",
            "\n",
            "Epoch 11/50\n",
            " - 100s - loss: 0.5435 - acc: 0.7526 - val_loss: 0.5280 - val_acc: 0.7002\n",
            "\n",
            "Epoch 12/50\n",
            " - 96s - loss: 0.5440 - acc: 0.7510 - val_loss: 0.5270 - val_acc: 0.7095\n",
            "\n",
            "Epoch 13/50\n",
            " - 96s - loss: 0.5461 - acc: 0.7504 - val_loss: 0.5099 - val_acc: 0.7075\n",
            "\n",
            "Epoch 14/50\n",
            " - 97s - loss: 0.5493 - acc: 0.7488 - val_loss: 0.5301 - val_acc: 0.6971\n",
            "\n",
            "Epoch 15/50\n",
            " - 96s - loss: 0.5483 - acc: 0.7467 - val_loss: 0.5119 - val_acc: 0.7204\n",
            "\n",
            "Epoch 16/50\n",
            " - 95s - loss: 0.5500 - acc: 0.7460 - val_loss: 0.5209 - val_acc: 0.7074\n",
            "\n",
            "Epoch 17/50\n",
            " - 97s - loss: 0.5504 - acc: 0.7459 - val_loss: 0.4945 - val_acc: 0.7348\n",
            "\n",
            "Epoch 18/50\n",
            " - 95s - loss: 0.5479 - acc: 0.7449 - val_loss: 0.5097 - val_acc: 0.7200\n",
            "\n",
            "Epoch 19/50\n",
            " - 95s - loss: 0.5503 - acc: 0.7457 - val_loss: 0.4980 - val_acc: 0.7240\n",
            "\n",
            "Epoch 20/50\n",
            " - 98s - loss: 0.5489 - acc: 0.7460 - val_loss: 0.4968 - val_acc: 0.7381\n",
            "\n",
            "Epoch 21/50\n",
            " - 96s - loss: 0.5472 - acc: 0.7461 - val_loss: 0.4845 - val_acc: 0.7615\n",
            "\n",
            "Epoch 22/50\n",
            " - 96s - loss: 0.5454 - acc: 0.7476 - val_loss: 0.4831 - val_acc: 0.7548\n",
            "\n",
            "Epoch 23/50\n",
            " - 97s - loss: 0.5494 - acc: 0.7456 - val_loss: 0.4827 - val_acc: 0.7735\n",
            "\n",
            "Epoch 24/50\n",
            " - 96s - loss: 0.5479 - acc: 0.7481 - val_loss: 0.4601 - val_acc: 0.7682\n",
            "\n",
            "Epoch 25/50\n",
            " - 95s - loss: 0.5452 - acc: 0.7479 - val_loss: 0.4727 - val_acc: 0.8073\n",
            "\n",
            "Epoch 26/50\n",
            " - 97s - loss: 0.5468 - acc: 0.7504 - val_loss: 0.4718 - val_acc: 0.7908\n",
            "\n",
            "Epoch 27/50\n",
            " - 97s - loss: 0.5446 - acc: 0.7505 - val_loss: 0.4673 - val_acc: 0.8083\n",
            "\n",
            "Epoch 28/50\n",
            " - 95s - loss: 0.5425 - acc: 0.7529 - val_loss: 0.4581 - val_acc: 0.8145\n",
            "\n",
            "Epoch 29/50\n",
            " - 96s - loss: 0.5427 - acc: 0.7539 - val_loss: 0.4470 - val_acc: 0.8093\n",
            "\n",
            "Epoch 30/50\n",
            " - 98s - loss: 0.5445 - acc: 0.7549 - val_loss: 0.4466 - val_acc: 0.8230\n",
            "\n",
            "Epoch 31/50\n",
            " - 97s - loss: 0.5395 - acc: 0.7586 - val_loss: 0.4345 - val_acc: 0.8239\n",
            "\n",
            "Epoch 32/50\n",
            " - 96s - loss: 0.5377 - acc: 0.7609 - val_loss: 0.4355 - val_acc: 0.8252\n",
            "\n",
            "Epoch 33/50\n",
            " - 98s - loss: 0.5374 - acc: 0.7634 - val_loss: 0.4323 - val_acc: 0.8204\n",
            "\n",
            "Epoch 34/50\n",
            " - 96s - loss: 0.5360 - acc: 0.7652 - val_loss: 0.4344 - val_acc: 0.8270\n",
            "\n",
            "Epoch 35/50\n",
            " - 96s - loss: 0.5362 - acc: 0.7655 - val_loss: 0.4322 - val_acc: 0.8187\n",
            "\n",
            "Epoch 36/50\n",
            " - 98s - loss: 0.5339 - acc: 0.7666 - val_loss: 0.4207 - val_acc: 0.8149\n",
            "\n",
            "Epoch 37/50\n",
            " - 96s - loss: 0.5353 - acc: 0.7688 - val_loss: 0.4208 - val_acc: 0.8234\n",
            "\n",
            "Epoch 38/50\n",
            " - 97s - loss: 0.5359 - acc: 0.7709 - val_loss: 0.4252 - val_acc: 0.8221\n",
            "\n",
            "Epoch 39/50\n",
            " - 97s - loss: 0.5369 - acc: 0.7697 - val_loss: 0.4172 - val_acc: 0.8187\n",
            "\n",
            "Epoch 40/50\n",
            " - 97s - loss: 0.5386 - acc: 0.7707 - val_loss: 0.4171 - val_acc: 0.8241\n",
            "\n",
            "Epoch 41/50\n",
            " - 96s - loss: 0.5416 - acc: 0.7704 - val_loss: 0.4303 - val_acc: 0.8226\n",
            "\n",
            "Epoch 42/50\n",
            " - 96s - loss: 0.5403 - acc: 0.7712 - val_loss: 0.4322 - val_acc: 0.8252\n",
            "\n",
            "Epoch 43/50\n",
            " - 99s - loss: 0.5409 - acc: 0.7714 - val_loss: 0.4258 - val_acc: 0.8250\n",
            "\n",
            "Epoch 44/50\n",
            " - 97s - loss: 0.5368 - acc: 0.7716 - val_loss: 0.4252 - val_acc: 0.8235\n",
            "\n",
            "Epoch 45/50\n",
            " - 97s - loss: 0.5421 - acc: 0.7721 - val_loss: 0.4260 - val_acc: 0.8297\n",
            "\n",
            "Epoch 46/50\n",
            " - 99s - loss: 0.5429 - acc: 0.7729 - val_loss: 0.4311 - val_acc: 0.8214\n",
            "\n",
            "Epoch 47/50\n",
            " - 96s - loss: 0.5442 - acc: 0.7730 - val_loss: 0.4348 - val_acc: 0.8161\n",
            "\n",
            "Epoch 48/50\n",
            " - 95s - loss: 0.5468 - acc: 0.7727 - val_loss: 0.4368 - val_acc: 0.8221\n",
            "\n",
            "Epoch 49/50\n",
            " - 96s - loss: 0.5454 - acc: 0.7729 - val_loss: 0.4363 - val_acc: 0.8218\n",
            "\n",
            "Epoch 50/50\n",
            " - 96s - loss: 0.5489 - acc: 0.7730 - val_loss: 0.4351 - val_acc: 0.8164\n",
            "\n",
            "Lowest Validation Loss:\n",
            "0.4170670696258545\n",
            "(1000000, 300)\n",
            " 40%|████      | 2/5 [2:38:11<3:55:42, 4714.00s/it, best loss: -0.4170670696258545]WARNING:tensorflow:Large dropout rate: 0.975819 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 50)                15050     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 500)               25500     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 2)                 1002      \n",
            "=================================================================\n",
            "Total params: 41,552\n",
            "Trainable params: 41,552\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 850000 samples, validate on 150000 samples\n",
            "Epoch 1/100\n",
            " - 42s - loss: 0.6156 - acc: 0.6353 - val_loss: 0.6691 - val_acc: 0.4955\n",
            "\n",
            "Epoch 2/100\n",
            " - 41s - loss: 0.5774 - acc: 0.6595 - val_loss: 0.6883 - val_acc: 0.4968\n",
            "\n",
            "Epoch 3/100\n",
            " - 41s - loss: 0.5722 - acc: 0.6637 - val_loss: 0.6797 - val_acc: 0.5784\n",
            "\n",
            "Epoch 4/100\n",
            " - 41s - loss: 0.5710 - acc: 0.6649 - val_loss: 0.6716 - val_acc: 0.5765\n",
            "\n",
            "Epoch 5/100\n",
            " - 41s - loss: 0.5687 - acc: 0.6662 - val_loss: 0.6692 - val_acc: 0.8173\n",
            "\n",
            "Epoch 6/100\n",
            " - 41s - loss: 0.5678 - acc: 0.6666 - val_loss: 0.6774 - val_acc: 0.7868\n",
            "\n",
            "Epoch 7/100\n",
            " - 41s - loss: 0.5670 - acc: 0.6679 - val_loss: 0.6711 - val_acc: 0.8247\n",
            "\n",
            "Epoch 8/100\n",
            " - 41s - loss: 0.5663 - acc: 0.6678 - val_loss: 0.6740 - val_acc: 0.8175\n",
            "\n",
            "Epoch 9/100\n",
            " - 40s - loss: 0.5656 - acc: 0.6686 - val_loss: 0.6796 - val_acc: 0.5847\n",
            "\n",
            "Epoch 10/100\n",
            " - 41s - loss: 0.5659 - acc: 0.6684 - val_loss: 0.6777 - val_acc: 0.8247\n",
            "\n",
            "Epoch 11/100\n",
            " - 41s - loss: 0.5657 - acc: 0.6694 - val_loss: 0.6849 - val_acc: 0.6053\n",
            "\n",
            "Epoch 12/100\n",
            " - 41s - loss: 0.5650 - acc: 0.6689 - val_loss: 0.6705 - val_acc: 0.8163\n",
            "\n",
            "Epoch 13/100\n",
            " - 41s - loss: 0.5646 - acc: 0.6692 - val_loss: 0.6605 - val_acc: 0.8230\n",
            "\n",
            "Epoch 14/100\n",
            " - 41s - loss: 0.5645 - acc: 0.6695 - val_loss: 0.6736 - val_acc: 0.6751\n",
            "\n",
            "Epoch 15/100\n",
            " - 41s - loss: 0.5640 - acc: 0.6700 - val_loss: 0.6756 - val_acc: 0.8186\n",
            "\n",
            "Epoch 16/100\n",
            " - 41s - loss: 0.5643 - acc: 0.6700 - val_loss: 0.6653 - val_acc: 0.8246\n",
            "\n",
            "Epoch 17/100\n",
            " - 40s - loss: 0.5641 - acc: 0.6694 - val_loss: 0.6724 - val_acc: 0.8223\n",
            "\n",
            "Epoch 18/100\n",
            " - 41s - loss: 0.5640 - acc: 0.6700 - val_loss: 0.6691 - val_acc: 0.8218\n",
            "\n",
            "Epoch 19/100\n",
            " - 41s - loss: 0.5638 - acc: 0.6702 - val_loss: 0.6798 - val_acc: 0.7670\n",
            "\n",
            "Epoch 20/100\n",
            " - 42s - loss: 0.5644 - acc: 0.6701 - val_loss: 0.6754 - val_acc: 0.5909\n",
            "\n",
            "Epoch 21/100\n",
            " - 41s - loss: 0.5646 - acc: 0.6696 - val_loss: 0.6714 - val_acc: 0.7956\n",
            "\n",
            "Epoch 22/100\n",
            " - 41s - loss: 0.5644 - acc: 0.6694 - val_loss: 0.6685 - val_acc: 0.8239\n",
            "\n",
            "Epoch 23/100\n",
            " - 41s - loss: 0.5640 - acc: 0.6697 - val_loss: 0.6458 - val_acc: 0.8244\n",
            "\n",
            "Epoch 24/100\n",
            " - 42s - loss: 0.5632 - acc: 0.6710 - val_loss: 0.6441 - val_acc: 0.8207\n",
            "\n",
            "Epoch 25/100\n",
            " - 41s - loss: 0.5640 - acc: 0.6697 - val_loss: 0.6737 - val_acc: 0.8233\n",
            "\n",
            "Epoch 26/100\n",
            " - 41s - loss: 0.5639 - acc: 0.6701 - val_loss: 0.6770 - val_acc: 0.8243\n",
            "\n",
            "Epoch 27/100\n",
            " - 42s - loss: 0.5626 - acc: 0.6707 - val_loss: 0.6869 - val_acc: 0.5130\n",
            "\n",
            "Epoch 28/100\n",
            " - 43s - loss: 0.5638 - acc: 0.6705 - val_loss: 0.6750 - val_acc: 0.8029\n",
            "\n",
            "Epoch 29/100\n",
            " - 41s - loss: 0.5632 - acc: 0.6706 - val_loss: 0.6580 - val_acc: 0.8212\n",
            "\n",
            "Epoch 30/100\n",
            " - 41s - loss: 0.5628 - acc: 0.6710 - val_loss: 0.6680 - val_acc: 0.8245\n",
            "\n",
            "Epoch 31/100\n",
            " - 41s - loss: 0.5634 - acc: 0.6709 - val_loss: 0.6500 - val_acc: 0.8226\n",
            "\n",
            "Epoch 32/100\n",
            " - 42s - loss: 0.5632 - acc: 0.6711 - val_loss: 0.6811 - val_acc: 0.8153\n",
            "\n",
            "Epoch 33/100\n",
            " - 42s - loss: 0.5629 - acc: 0.6711 - val_loss: 0.6678 - val_acc: 0.8244\n",
            "\n",
            "Epoch 34/100\n",
            " - 42s - loss: 0.5632 - acc: 0.6710 - val_loss: 0.6747 - val_acc: 0.7712\n",
            "\n",
            "Epoch 35/100\n",
            " - 42s - loss: 0.5629 - acc: 0.6711 - val_loss: 0.6629 - val_acc: 0.8261\n",
            "\n",
            "Epoch 36/100\n",
            " - 41s - loss: 0.5626 - acc: 0.6715 - val_loss: 0.6810 - val_acc: 0.8215\n",
            "\n",
            "Epoch 37/100\n",
            " - 41s - loss: 0.5631 - acc: 0.6708 - val_loss: 0.6762 - val_acc: 0.7923\n",
            "\n",
            "Epoch 38/100\n",
            " - 41s - loss: 0.5634 - acc: 0.6703 - val_loss: 0.6587 - val_acc: 0.8276\n",
            "\n",
            "Epoch 39/100\n",
            " - 42s - loss: 0.5625 - acc: 0.6717 - val_loss: 0.6669 - val_acc: 0.8280\n",
            "\n",
            "Epoch 40/100\n",
            " - 42s - loss: 0.5625 - acc: 0.6714 - val_loss: 0.6555 - val_acc: 0.8166\n",
            "\n",
            "Epoch 41/100\n",
            " - 42s - loss: 0.5626 - acc: 0.6716 - val_loss: 0.6778 - val_acc: 0.8224\n",
            "\n",
            "Epoch 42/100\n",
            " - 42s - loss: 0.5627 - acc: 0.6711 - val_loss: 0.6533 - val_acc: 0.5507\n",
            "\n",
            "Epoch 43/100\n",
            " - 42s - loss: 0.5625 - acc: 0.6712 - val_loss: 0.6718 - val_acc: 0.8248\n",
            "\n",
            "Epoch 44/100\n",
            " - 42s - loss: 0.5628 - acc: 0.6710 - val_loss: 0.6608 - val_acc: 0.8121\n",
            "\n",
            "Epoch 45/100\n",
            " - 41s - loss: 0.5634 - acc: 0.6709 - val_loss: 0.6870 - val_acc: 0.5377\n",
            "\n",
            "Epoch 46/100\n",
            " - 41s - loss: 0.5624 - acc: 0.6714 - val_loss: 0.6624 - val_acc: 0.8247\n",
            "\n",
            "Epoch 47/100\n",
            " - 42s - loss: 0.5621 - acc: 0.6712 - val_loss: 0.6724 - val_acc: 0.8207\n",
            "\n",
            "Epoch 48/100\n",
            " - 42s - loss: 0.5623 - acc: 0.6711 - val_loss: 0.6529 - val_acc: 0.8268\n",
            "\n",
            "Epoch 49/100\n",
            " - 42s - loss: 0.5622 - acc: 0.6717 - val_loss: 0.6596 - val_acc: 0.8228\n",
            "\n",
            "Epoch 50/100\n",
            " - 43s - loss: 0.5632 - acc: 0.6706 - val_loss: 0.6497 - val_acc: 0.8261\n",
            "\n",
            "Epoch 51/100\n",
            " - 41s - loss: 0.5630 - acc: 0.6707 - val_loss: 0.6555 - val_acc: 0.8241\n",
            "\n",
            "Epoch 52/100\n",
            " - 41s - loss: 0.5630 - acc: 0.6709 - val_loss: 0.6558 - val_acc: 0.8204\n",
            "\n",
            "Epoch 53/100\n",
            " - 41s - loss: 0.5629 - acc: 0.6713 - val_loss: 0.6712 - val_acc: 0.8242\n",
            "\n",
            "Epoch 54/100\n",
            " - 41s - loss: 0.5625 - acc: 0.6708 - val_loss: 0.6490 - val_acc: 0.8253\n",
            "\n",
            "Epoch 55/100\n",
            " - 41s - loss: 0.5628 - acc: 0.6711 - val_loss: 0.6564 - val_acc: 0.8260\n",
            "\n",
            "Epoch 56/100\n",
            " - 41s - loss: 0.5633 - acc: 0.6706 - val_loss: 0.6662 - val_acc: 0.8229\n",
            "\n",
            "Epoch 57/100\n",
            " - 41s - loss: 0.5620 - acc: 0.6711 - val_loss: 0.6745 - val_acc: 0.6496\n",
            "\n",
            "Epoch 58/100\n",
            " - 41s - loss: 0.5634 - acc: 0.6707 - val_loss: 0.6736 - val_acc: 0.8151\n",
            "\n",
            "Epoch 59/100\n",
            " - 41s - loss: 0.5627 - acc: 0.6712 - val_loss: 0.6637 - val_acc: 0.8247\n",
            "\n",
            "Epoch 60/100\n",
            " - 41s - loss: 0.5629 - acc: 0.6709 - val_loss: 0.6708 - val_acc: 0.8224\n",
            "\n",
            "Epoch 61/100\n",
            " - 41s - loss: 0.5631 - acc: 0.6709 - val_loss: 0.6845 - val_acc: 0.6340\n",
            "\n",
            "Epoch 62/100\n",
            " - 42s - loss: 0.5623 - acc: 0.6717 - val_loss: 0.6751 - val_acc: 0.8259\n",
            "\n",
            "Epoch 63/100\n",
            " - 42s - loss: 0.5620 - acc: 0.6718 - val_loss: 0.6595 - val_acc: 0.8236\n",
            "\n",
            "Epoch 64/100\n",
            " - 41s - loss: 0.5623 - acc: 0.6717 - val_loss: 0.6782 - val_acc: 0.8249\n",
            "\n",
            "Epoch 65/100\n",
            " - 42s - loss: 0.5628 - acc: 0.6710 - val_loss: 0.6572 - val_acc: 0.8203\n",
            "\n",
            "Epoch 66/100\n",
            " - 41s - loss: 0.5630 - acc: 0.6710 - val_loss: 0.6697 - val_acc: 0.8205\n",
            "\n",
            "Epoch 67/100\n",
            " - 41s - loss: 0.5624 - acc: 0.6713 - val_loss: 0.6616 - val_acc: 0.8254\n",
            "\n",
            "Epoch 68/100\n",
            " - 41s - loss: 0.5628 - acc: 0.6716 - val_loss: 0.6734 - val_acc: 0.8058\n",
            "\n",
            "Epoch 69/100\n",
            " - 41s - loss: 0.5624 - acc: 0.6717 - val_loss: 0.6605 - val_acc: 0.8200\n",
            "\n",
            "Epoch 70/100\n",
            " - 41s - loss: 0.5629 - acc: 0.6717 - val_loss: 0.6764 - val_acc: 0.7737\n",
            "\n",
            "Epoch 71/100\n",
            " - 41s - loss: 0.5629 - acc: 0.6711 - val_loss: 0.6620 - val_acc: 0.8215\n",
            "\n",
            "Epoch 72/100\n",
            " - 42s - loss: 0.5632 - acc: 0.6710 - val_loss: 0.6638 - val_acc: 0.8236\n",
            "\n",
            "Epoch 73/100\n",
            " - 42s - loss: 0.5620 - acc: 0.6712 - val_loss: 0.6449 - val_acc: 0.8241\n",
            "\n",
            "Epoch 74/100\n",
            " - 42s - loss: 0.5631 - acc: 0.6714 - val_loss: 0.6534 - val_acc: 0.8204\n",
            "\n",
            "Epoch 75/100\n",
            " - 42s - loss: 0.5625 - acc: 0.6713 - val_loss: 0.6529 - val_acc: 0.6165\n",
            "\n",
            "Epoch 76/100\n",
            " - 42s - loss: 0.5626 - acc: 0.6713 - val_loss: 0.6724 - val_acc: 0.6034\n",
            "\n",
            "Epoch 77/100\n",
            " - 42s - loss: 0.5620 - acc: 0.6714 - val_loss: 0.6746 - val_acc: 0.8248\n",
            "\n",
            "Epoch 78/100\n",
            " - 42s - loss: 0.5622 - acc: 0.6719 - val_loss: 0.6658 - val_acc: 0.8212\n",
            "\n",
            "Epoch 79/100\n",
            " - 42s - loss: 0.5634 - acc: 0.6707 - val_loss: 0.6715 - val_acc: 0.7414\n",
            "\n",
            "Epoch 80/100\n",
            " - 43s - loss: 0.5632 - acc: 0.6708 - val_loss: 0.6519 - val_acc: 0.8196\n",
            "\n",
            "Epoch 81/100\n",
            " - 42s - loss: 0.5621 - acc: 0.6713 - val_loss: 0.6674 - val_acc: 0.8297\n",
            "\n",
            "Epoch 82/100\n",
            " - 43s - loss: 0.5627 - acc: 0.6709 - val_loss: 0.6355 - val_acc: 0.8236\n",
            "\n",
            "Epoch 83/100\n",
            " - 42s - loss: 0.5629 - acc: 0.6710 - val_loss: 0.6599 - val_acc: 0.8209\n",
            "\n",
            "Epoch 84/100\n",
            " - 43s - loss: 0.5626 - acc: 0.6709 - val_loss: 0.6470 - val_acc: 0.8241\n",
            "\n",
            "Epoch 85/100\n",
            " - 43s - loss: 0.5632 - acc: 0.6707 - val_loss: 0.6623 - val_acc: 0.8238\n",
            "\n",
            "Epoch 86/100\n",
            " - 42s - loss: 0.5631 - acc: 0.6710 - val_loss: 0.6397 - val_acc: 0.8241\n",
            "\n",
            "Epoch 87/100\n",
            " - 43s - loss: 0.5623 - acc: 0.6718 - val_loss: 0.6673 - val_acc: 0.8212\n",
            "\n",
            "Epoch 88/100\n",
            " - 42s - loss: 0.5623 - acc: 0.6718 - val_loss: 0.6555 - val_acc: 0.8226\n",
            "\n",
            "Epoch 89/100\n",
            " - 43s - loss: 0.5626 - acc: 0.6717 - val_loss: 0.6830 - val_acc: 0.7904\n",
            "\n",
            "Epoch 90/100\n",
            " - 43s - loss: 0.5624 - acc: 0.6716 - val_loss: 0.6613 - val_acc: 0.8215\n",
            "\n",
            "Epoch 91/100\n",
            " - 42s - loss: 0.5628 - acc: 0.6714 - val_loss: 0.6537 - val_acc: 0.8221\n",
            "\n",
            "Epoch 92/100\n",
            " - 42s - loss: 0.5625 - acc: 0.6716 - val_loss: 0.6639 - val_acc: 0.8232\n",
            "\n",
            "Epoch 93/100\n",
            " - 42s - loss: 0.5627 - acc: 0.6711 - val_loss: 0.6539 - val_acc: 0.8235\n",
            "\n",
            "Epoch 94/100\n",
            " - 42s - loss: 0.5625 - acc: 0.6714 - val_loss: 0.6722 - val_acc: 0.8190\n",
            "\n",
            "Epoch 95/100\n",
            " - 43s - loss: 0.5624 - acc: 0.6714 - val_loss: 0.6603 - val_acc: 0.8203\n",
            "\n",
            "Epoch 96/100\n",
            " - 42s - loss: 0.5620 - acc: 0.6715 - val_loss: 0.6683 - val_acc: 0.7879\n",
            "\n",
            "Epoch 97/100\n",
            " - 42s - loss: 0.5624 - acc: 0.6716 - val_loss: 0.6455 - val_acc: 0.8201\n",
            "\n",
            "Epoch 98/100\n",
            " - 41s - loss: 0.5621 - acc: 0.6716 - val_loss: 0.6855 - val_acc: 0.7287\n",
            "\n",
            "Epoch 99/100\n",
            " - 42s - loss: 0.5625 - acc: 0.6712 - val_loss: 0.6575 - val_acc: 0.7931\n",
            "\n",
            "Epoch 100/100\n",
            " - 42s - loss: 0.5628 - acc: 0.6706 - val_loss: 0.6725 - val_acc: 0.6627\n",
            "\n",
            "Lowest Validation Loss:\n",
            "0.6354889180819193\n",
            "(1000000, 300)\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 500)               150500    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 50)                25050     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 2)                 102       \n",
            "=================================================================\n",
            "Total params: 175,652\n",
            "Trainable params: 175,652\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 850000 samples, validate on 150000 samples\n",
            "Epoch 1/100\n",
            " - 73s - loss: 0.4245 - acc: 0.8136 - val_loss: 0.3703 - val_acc: 0.8389\n",
            "\n",
            "Epoch 2/100\n",
            " - 74s - loss: 0.3922 - acc: 0.8313 - val_loss: 0.3630 - val_acc: 0.8419\n",
            "\n",
            "Epoch 3/100\n",
            " - 77s - loss: 0.3870 - acc: 0.8346 - val_loss: 0.3616 - val_acc: 0.8393\n",
            "\n",
            "Epoch 4/100\n",
            " - 76s - loss: 0.3833 - acc: 0.8363 - val_loss: 0.3594 - val_acc: 0.8458\n",
            "\n",
            "Epoch 5/100\n",
            " - 78s - loss: 0.3806 - acc: 0.8375 - val_loss: 0.3592 - val_acc: 0.8463\n",
            "\n",
            "Epoch 6/100\n",
            " - 75s - loss: 0.3789 - acc: 0.8383 - val_loss: 0.3616 - val_acc: 0.8424\n",
            "\n",
            "Epoch 7/100\n",
            " - 74s - loss: 0.3772 - acc: 0.8395 - val_loss: 0.3572 - val_acc: 0.8458\n",
            "\n",
            "Epoch 8/100\n",
            " - 76s - loss: 0.3757 - acc: 0.8407 - val_loss: 0.3531 - val_acc: 0.8488\n",
            "\n",
            "Epoch 9/100\n",
            " - 75s - loss: 0.3742 - acc: 0.8408 - val_loss: 0.3568 - val_acc: 0.8433\n",
            "\n",
            "Epoch 10/100\n",
            " - 74s - loss: 0.3732 - acc: 0.8416 - val_loss: 0.3494 - val_acc: 0.8483\n",
            "\n",
            "Epoch 11/100\n",
            " - 74s - loss: 0.3719 - acc: 0.8420 - val_loss: 0.3490 - val_acc: 0.8503\n",
            "\n",
            "Epoch 12/100\n",
            " - 72s - loss: 0.3711 - acc: 0.8429 - val_loss: 0.3501 - val_acc: 0.8500\n",
            "\n",
            "Epoch 13/100\n",
            " - 73s - loss: 0.3708 - acc: 0.8431 - val_loss: 0.3505 - val_acc: 0.8474\n",
            "\n",
            "Epoch 14/100\n",
            " - 77s - loss: 0.3695 - acc: 0.8432 - val_loss: 0.3486 - val_acc: 0.8484\n",
            "\n",
            "Epoch 15/100\n",
            " - 75s - loss: 0.3691 - acc: 0.8437 - val_loss: 0.3453 - val_acc: 0.8509\n",
            "\n",
            "Epoch 16/100\n",
            " - 73s - loss: 0.3687 - acc: 0.8436 - val_loss: 0.3488 - val_acc: 0.8487\n",
            "\n",
            "Epoch 17/100\n",
            " - 72s - loss: 0.3675 - acc: 0.8449 - val_loss: 0.3454 - val_acc: 0.8490\n",
            "\n",
            "Epoch 18/100\n",
            " - 75s - loss: 0.3676 - acc: 0.8445 - val_loss: 0.3532 - val_acc: 0.8518\n",
            "\n",
            "Epoch 19/100\n",
            " - 73s - loss: 0.3671 - acc: 0.8445 - val_loss: 0.3493 - val_acc: 0.8493\n",
            "\n",
            "Epoch 20/100\n",
            " - 75s - loss: 0.3667 - acc: 0.8444 - val_loss: 0.3446 - val_acc: 0.8513\n",
            "\n",
            "Epoch 21/100\n",
            " - 72s - loss: 0.3660 - acc: 0.8455 - val_loss: 0.3432 - val_acc: 0.8525\n",
            "\n",
            "Epoch 22/100\n",
            " - 76s - loss: 0.3656 - acc: 0.8457 - val_loss: 0.3435 - val_acc: 0.8530\n",
            "\n",
            "Epoch 23/100\n",
            " - 74s - loss: 0.3656 - acc: 0.8453 - val_loss: 0.3433 - val_acc: 0.8528\n",
            "\n",
            "Epoch 24/100\n",
            " - 71s - loss: 0.3645 - acc: 0.8458 - val_loss: 0.3532 - val_acc: 0.8493\n",
            "\n",
            "Epoch 25/100\n",
            " - 71s - loss: 0.3650 - acc: 0.8457 - val_loss: 0.3553 - val_acc: 0.8522\n",
            "\n",
            "Epoch 26/100\n",
            " - 72s - loss: 0.3644 - acc: 0.8464 - val_loss: 0.3410 - val_acc: 0.8531\n",
            "\n",
            "Epoch 27/100\n",
            " - 70s - loss: 0.3637 - acc: 0.8465 - val_loss: 0.3469 - val_acc: 0.8537\n",
            "\n",
            "Epoch 28/100\n",
            " - 70s - loss: 0.3636 - acc: 0.8463 - val_loss: 0.3529 - val_acc: 0.8505\n",
            "\n",
            "Epoch 29/100\n",
            " - 72s - loss: 0.3637 - acc: 0.8462 - val_loss: 0.3446 - val_acc: 0.8549\n",
            "\n",
            "Epoch 30/100\n",
            " - 71s - loss: 0.3631 - acc: 0.8471 - val_loss: 0.3534 - val_acc: 0.8498\n",
            "\n",
            "Epoch 31/100\n",
            " - 74s - loss: 0.3632 - acc: 0.8467 - val_loss: 0.3495 - val_acc: 0.8525\n",
            "\n",
            "Epoch 32/100\n",
            " - 73s - loss: 0.3628 - acc: 0.8468 - val_loss: 0.3433 - val_acc: 0.8536\n",
            "\n",
            "Epoch 33/100\n",
            " - 74s - loss: 0.3630 - acc: 0.8471 - val_loss: 0.3492 - val_acc: 0.8481\n",
            "\n",
            "Epoch 34/100\n",
            " - 76s - loss: 0.3621 - acc: 0.8472 - val_loss: 0.3419 - val_acc: 0.8517\n",
            "\n",
            "Epoch 35/100\n",
            " - 71s - loss: 0.3624 - acc: 0.8480 - val_loss: 0.3437 - val_acc: 0.8495\n",
            "\n",
            "Epoch 36/100\n",
            " - 72s - loss: 0.3623 - acc: 0.8477 - val_loss: 0.3456 - val_acc: 0.8521\n",
            "\n",
            "Epoch 37/100\n",
            " - 74s - loss: 0.3615 - acc: 0.8472 - val_loss: 0.3411 - val_acc: 0.8528\n",
            "\n",
            "Epoch 38/100\n",
            " - 72s - loss: 0.3616 - acc: 0.8477 - val_loss: 0.3437 - val_acc: 0.8537\n",
            "\n",
            "Epoch 39/100\n",
            " - 74s - loss: 0.3610 - acc: 0.8479 - val_loss: 0.3447 - val_acc: 0.8552\n",
            "\n",
            "Epoch 40/100\n",
            " - 73s - loss: 0.3615 - acc: 0.8479 - val_loss: 0.3378 - val_acc: 0.8551\n",
            "\n",
            "Epoch 41/100\n",
            " - 77s - loss: 0.3617 - acc: 0.8480 - val_loss: 0.3463 - val_acc: 0.8537\n",
            "\n",
            "Epoch 42/100\n",
            " - 74s - loss: 0.3615 - acc: 0.8479 - val_loss: 0.3464 - val_acc: 0.8513\n",
            "\n",
            "Epoch 43/100\n",
            " - 74s - loss: 0.3606 - acc: 0.8478 - val_loss: 0.3476 - val_acc: 0.8522\n",
            "\n",
            "Epoch 44/100\n",
            " - 75s - loss: 0.3617 - acc: 0.8481 - val_loss: 0.3399 - val_acc: 0.8539\n",
            "\n",
            "Epoch 45/100\n",
            " - 74s - loss: 0.3604 - acc: 0.8483 - val_loss: 0.3436 - val_acc: 0.8540\n",
            "\n",
            "Epoch 46/100\n",
            " - 74s - loss: 0.3606 - acc: 0.8483 - val_loss: 0.3409 - val_acc: 0.8557\n",
            "\n",
            "Epoch 47/100\n",
            " - 69s - loss: 0.3602 - acc: 0.8481 - val_loss: 0.3392 - val_acc: 0.8543\n",
            "\n",
            "Epoch 48/100\n",
            " - 74s - loss: 0.3607 - acc: 0.8482 - val_loss: 0.3381 - val_acc: 0.8541\n",
            "\n",
            "Epoch 49/100\n",
            " - 72s - loss: 0.3601 - acc: 0.8485 - val_loss: 0.3421 - val_acc: 0.8563\n",
            "\n",
            "Epoch 50/100\n",
            " - 70s - loss: 0.3594 - acc: 0.8488 - val_loss: 0.3436 - val_acc: 0.8541\n",
            "\n",
            "Epoch 51/100\n",
            " - 72s - loss: 0.3597 - acc: 0.8485 - val_loss: 0.3471 - val_acc: 0.8531\n",
            "\n",
            "Epoch 52/100\n",
            " - 73s - loss: 0.3595 - acc: 0.8485 - val_loss: 0.3391 - val_acc: 0.8539\n",
            "\n",
            "Epoch 53/100\n",
            " - 73s - loss: 0.3597 - acc: 0.8488 - val_loss: 0.3396 - val_acc: 0.8511\n",
            "\n",
            "Epoch 54/100\n",
            " - 75s - loss: 0.3597 - acc: 0.8491 - val_loss: 0.3449 - val_acc: 0.8555\n",
            "\n",
            "Epoch 55/100\n",
            " - 75s - loss: 0.3590 - acc: 0.8489 - val_loss: 0.3465 - val_acc: 0.8511\n",
            "\n",
            "Epoch 56/100\n",
            " - 73s - loss: 0.3589 - acc: 0.8492 - val_loss: 0.3383 - val_acc: 0.8543\n",
            "\n",
            "Epoch 57/100\n",
            " - 72s - loss: 0.3597 - acc: 0.8487 - val_loss: 0.3380 - val_acc: 0.8524\n",
            "\n",
            "Epoch 58/100\n",
            " - 74s - loss: 0.3585 - acc: 0.8493 - val_loss: 0.3452 - val_acc: 0.8525\n",
            "\n",
            "Epoch 59/100\n",
            " - 74s - loss: 0.3590 - acc: 0.8492 - val_loss: 0.3372 - val_acc: 0.8548\n",
            "\n",
            "Epoch 60/100\n",
            " - 74s - loss: 0.3585 - acc: 0.8492 - val_loss: 0.3437 - val_acc: 0.8476\n",
            "\n",
            "Epoch 61/100\n",
            " - 79s - loss: 0.3591 - acc: 0.8487 - val_loss: 0.3404 - val_acc: 0.8519\n",
            "\n",
            "Epoch 62/100\n",
            " - 74s - loss: 0.3580 - acc: 0.8496 - val_loss: 0.3380 - val_acc: 0.8541\n",
            "\n",
            "Epoch 63/100\n",
            " - 71s - loss: 0.3585 - acc: 0.8496 - val_loss: 0.3386 - val_acc: 0.8525\n",
            "\n",
            "Epoch 64/100\n",
            " - 73s - loss: 0.3589 - acc: 0.8492 - val_loss: 0.3364 - val_acc: 0.8537\n",
            "\n",
            "Epoch 65/100\n",
            " - 75s - loss: 0.3581 - acc: 0.8500 - val_loss: 0.3399 - val_acc: 0.8521\n",
            "\n",
            "Epoch 66/100\n",
            " - 73s - loss: 0.3587 - acc: 0.8494 - val_loss: 0.3478 - val_acc: 0.8483\n",
            "\n",
            "Epoch 67/100\n",
            " - 72s - loss: 0.3577 - acc: 0.8497 - val_loss: 0.3380 - val_acc: 0.8509\n",
            "\n",
            "Epoch 68/100\n",
            " - 71s - loss: 0.3579 - acc: 0.8497 - val_loss: 0.3393 - val_acc: 0.8498\n",
            "\n",
            "Epoch 69/100\n",
            " - 74s - loss: 0.3581 - acc: 0.8496 - val_loss: 0.3387 - val_acc: 0.8508\n",
            "\n",
            "Epoch 70/100\n",
            " - 72s - loss: 0.3571 - acc: 0.8501 - val_loss: 0.3420 - val_acc: 0.8523\n",
            "\n",
            "Epoch 71/100\n",
            " - 74s - loss: 0.3580 - acc: 0.8503 - val_loss: 0.3393 - val_acc: 0.8494\n",
            "\n",
            "Epoch 72/100\n",
            " - 74s - loss: 0.3575 - acc: 0.8501 - val_loss: 0.3426 - val_acc: 0.8539\n",
            "\n",
            "Epoch 73/100\n",
            " - 74s - loss: 0.3578 - acc: 0.8496 - val_loss: 0.3401 - val_acc: 0.8548\n",
            "\n",
            "Epoch 74/100\n",
            " - 73s - loss: 0.3575 - acc: 0.8500 - val_loss: 0.3392 - val_acc: 0.8538\n",
            "\n",
            "Epoch 75/100\n",
            " - 76s - loss: 0.3575 - acc: 0.8495 - val_loss: 0.3397 - val_acc: 0.8540\n",
            "\n",
            "Epoch 76/100\n",
            " - 82s - loss: 0.3571 - acc: 0.8499 - val_loss: 0.3367 - val_acc: 0.8561\n",
            "\n",
            "Epoch 77/100\n",
            " - 81s - loss: 0.3572 - acc: 0.8503 - val_loss: 0.3387 - val_acc: 0.8549\n",
            "\n",
            "Epoch 78/100\n",
            " - 74s - loss: 0.3572 - acc: 0.8503 - val_loss: 0.3482 - val_acc: 0.8526\n",
            "\n",
            "Epoch 79/100\n",
            " - 76s - loss: 0.3574 - acc: 0.8503 - val_loss: 0.3379 - val_acc: 0.8526\n",
            "\n",
            "Epoch 80/100\n",
            " - 76s - loss: 0.3572 - acc: 0.8502 - val_loss: 0.3392 - val_acc: 0.8556\n",
            "\n",
            "Epoch 81/100\n",
            " - 78s - loss: 0.3573 - acc: 0.8503 - val_loss: 0.3407 - val_acc: 0.8511\n",
            "\n",
            "Epoch 82/100\n",
            " - 76s - loss: 0.3567 - acc: 0.8502 - val_loss: 0.3405 - val_acc: 0.8543\n",
            "\n",
            "Epoch 83/100\n",
            " - 77s - loss: 0.3569 - acc: 0.8505 - val_loss: 0.3422 - val_acc: 0.8525\n",
            "\n",
            "Epoch 84/100\n",
            " - 76s - loss: 0.3567 - acc: 0.8506 - val_loss: 0.3412 - val_acc: 0.8543\n",
            "\n",
            "Epoch 85/100\n",
            " - 77s - loss: 0.3564 - acc: 0.8505 - val_loss: 0.3357 - val_acc: 0.8537\n",
            "\n",
            "Epoch 86/100\n",
            " - 75s - loss: 0.3564 - acc: 0.8507 - val_loss: 0.3387 - val_acc: 0.8548\n",
            "\n",
            "Epoch 87/100\n",
            " - 75s - loss: 0.3563 - acc: 0.8508 - val_loss: 0.3433 - val_acc: 0.8547\n",
            "\n",
            "Epoch 88/100\n",
            " - 76s - loss: 0.3568 - acc: 0.8505 - val_loss: 0.3370 - val_acc: 0.8519\n",
            "\n",
            "Epoch 89/100\n",
            " - 77s - loss: 0.3563 - acc: 0.8510 - val_loss: 0.3424 - val_acc: 0.8563\n",
            "\n",
            "Epoch 90/100\n",
            " - 78s - loss: 0.3555 - acc: 0.8506 - val_loss: 0.3411 - val_acc: 0.8509\n",
            "\n",
            "Epoch 91/100\n",
            " - 76s - loss: 0.3558 - acc: 0.8507 - val_loss: 0.3394 - val_acc: 0.8521\n",
            "\n",
            "Epoch 92/100\n",
            " - 76s - loss: 0.3558 - acc: 0.8509 - val_loss: 0.3392 - val_acc: 0.8561\n",
            "\n",
            "Epoch 93/100\n",
            " - 75s - loss: 0.3556 - acc: 0.8506 - val_loss: 0.3438 - val_acc: 0.8536\n",
            "\n",
            "Epoch 94/100\n",
            " - 77s - loss: 0.3565 - acc: 0.8505 - val_loss: 0.3371 - val_acc: 0.8567\n",
            "\n",
            "Epoch 95/100\n",
            " - 75s - loss: 0.3559 - acc: 0.8509 - val_loss: 0.3409 - val_acc: 0.8553\n",
            "\n",
            "Epoch 96/100\n",
            " - 72s - loss: 0.3560 - acc: 0.8511 - val_loss: 0.3399 - val_acc: 0.8556\n",
            "\n",
            "Epoch 97/100\n",
            " - 75s - loss: 0.3556 - acc: 0.8509 - val_loss: 0.3389 - val_acc: 0.8521\n",
            "\n",
            "Epoch 98/100\n",
            " - 77s - loss: 0.3559 - acc: 0.8508 - val_loss: 0.3484 - val_acc: 0.8476\n",
            "\n",
            "Epoch 99/100\n",
            " - 78s - loss: 0.3562 - acc: 0.8506 - val_loss: 0.3464 - val_acc: 0.8533\n",
            "\n",
            "Epoch 100/100\n",
            " - 79s - loss: 0.3557 - acc: 0.8509 - val_loss: 0.3416 - val_acc: 0.8545\n",
            "\n",
            "Lowest Validation Loss:\n",
            "0.3357452492396037\n",
            "(1000000, 300)\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_13 (Dense)             (None, 500)               150500    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 200)               100200    \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 2)                 402       \n",
            "=================================================================\n",
            "Total params: 251,102\n",
            "Trainable params: 251,102\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 850000 samples, validate on 150000 samples\n",
            "Epoch 1/50\n",
            " - 93s - loss: 0.3567 - acc: 0.8440 - val_loss: 0.3372 - val_acc: 0.8533\n",
            "\n",
            "Epoch 2/50\n",
            " - 90s - loss: 0.3352 - acc: 0.8551 - val_loss: 0.3312 - val_acc: 0.8565\n",
            "\n",
            "Epoch 3/50\n",
            " - 90s - loss: 0.3257 - acc: 0.8598 - val_loss: 0.3239 - val_acc: 0.8602\n",
            "\n",
            "Epoch 4/50\n",
            " - 87s - loss: 0.3188 - acc: 0.8630 - val_loss: 0.3195 - val_acc: 0.8618\n",
            "\n",
            "Epoch 5/50\n",
            " - 90s - loss: 0.3131 - acc: 0.8652 - val_loss: 0.3193 - val_acc: 0.8621\n",
            "\n",
            "Epoch 6/50\n",
            " - 91s - loss: 0.3086 - acc: 0.8675 - val_loss: 0.3175 - val_acc: 0.8633\n",
            "\n",
            "Epoch 7/50\n",
            " - 89s - loss: 0.3035 - acc: 0.8693 - val_loss: 0.3245 - val_acc: 0.8633\n",
            "\n",
            "Epoch 8/50\n",
            " - 87s - loss: 0.2998 - acc: 0.8713 - val_loss: 0.3171 - val_acc: 0.8645\n",
            "\n",
            "Epoch 9/50\n",
            " - 89s - loss: 0.2959 - acc: 0.8727 - val_loss: 0.3176 - val_acc: 0.8651\n",
            "\n",
            "Epoch 10/50\n",
            " - 88s - loss: 0.2924 - acc: 0.8744 - val_loss: 0.3176 - val_acc: 0.8650\n",
            "\n",
            "Epoch 11/50\n",
            " - 89s - loss: 0.2887 - acc: 0.8757 - val_loss: 0.3198 - val_acc: 0.8650\n",
            "\n",
            "Epoch 12/50\n",
            " - 93s - loss: 0.2861 - acc: 0.8767 - val_loss: 0.3218 - val_acc: 0.8640\n",
            "\n",
            "Epoch 13/50\n",
            " - 89s - loss: 0.2830 - acc: 0.8779 - val_loss: 0.3194 - val_acc: 0.8643\n",
            "\n",
            "Epoch 14/50\n",
            " - 87s - loss: 0.2800 - acc: 0.8792 - val_loss: 0.3216 - val_acc: 0.8642\n",
            "\n",
            "Epoch 15/50\n",
            " - 91s - loss: 0.2768 - acc: 0.8804 - val_loss: 0.3214 - val_acc: 0.8642\n",
            "\n",
            "Epoch 16/50\n",
            " - 91s - loss: 0.2746 - acc: 0.8816 - val_loss: 0.3218 - val_acc: 0.8647\n",
            "\n",
            "Epoch 17/50\n",
            " - 89s - loss: 0.2721 - acc: 0.8825 - val_loss: 0.3244 - val_acc: 0.8646\n",
            "\n",
            "Epoch 18/50\n",
            " - 91s - loss: 0.2695 - acc: 0.8830 - val_loss: 0.3246 - val_acc: 0.8650\n",
            "\n",
            "Epoch 19/50\n",
            " - 91s - loss: 0.2672 - acc: 0.8844 - val_loss: 0.3261 - val_acc: 0.8647\n",
            "\n",
            "Epoch 20/50\n",
            " - 90s - loss: 0.2649 - acc: 0.8849 - val_loss: 0.3280 - val_acc: 0.8632\n",
            "\n",
            "Epoch 21/50\n",
            " - 92s - loss: 0.2629 - acc: 0.8861 - val_loss: 0.3251 - val_acc: 0.8636\n",
            "\n",
            "Epoch 22/50\n",
            " - 94s - loss: 0.2607 - acc: 0.8868 - val_loss: 0.3319 - val_acc: 0.8628\n",
            "\n",
            "Epoch 23/50\n",
            " - 93s - loss: 0.2590 - acc: 0.8875 - val_loss: 0.3295 - val_acc: 0.8636\n",
            "\n",
            "Epoch 24/50\n",
            " - 89s - loss: 0.2570 - acc: 0.8884 - val_loss: 0.3334 - val_acc: 0.8630\n",
            "\n",
            "Epoch 25/50\n",
            " - 96s - loss: 0.2554 - acc: 0.8888 - val_loss: 0.3335 - val_acc: 0.8622\n",
            "\n",
            "Epoch 26/50\n",
            " - 93s - loss: 0.2534 - acc: 0.8901 - val_loss: 0.3417 - val_acc: 0.8620\n",
            "\n",
            "Epoch 27/50\n",
            " - 89s - loss: 0.2515 - acc: 0.8905 - val_loss: 0.3402 - val_acc: 0.8620\n",
            "\n",
            "Epoch 28/50\n",
            " - 88s - loss: 0.2496 - acc: 0.8909 - val_loss: 0.3347 - val_acc: 0.8622\n",
            "\n",
            "Epoch 29/50\n",
            " - 91s - loss: 0.2481 - acc: 0.8917 - val_loss: 0.3383 - val_acc: 0.8612\n",
            "\n",
            "Epoch 30/50\n",
            " - 88s - loss: 0.2467 - acc: 0.8926 - val_loss: 0.3385 - val_acc: 0.8614\n",
            "\n",
            "Epoch 31/50\n",
            " - 84s - loss: 0.2451 - acc: 0.8931 - val_loss: 0.3470 - val_acc: 0.8626\n",
            "\n",
            "Epoch 32/50\n",
            " - 90s - loss: 0.2434 - acc: 0.8935 - val_loss: 0.3433 - val_acc: 0.8611\n",
            "\n",
            "Epoch 33/50\n",
            " - 91s - loss: 0.2420 - acc: 0.8940 - val_loss: 0.3413 - val_acc: 0.8604\n",
            "\n",
            "Epoch 34/50\n",
            " - 93s - loss: 0.2404 - acc: 0.8949 - val_loss: 0.3462 - val_acc: 0.8613\n",
            "\n",
            "Epoch 35/50\n",
            " - 96s - loss: 0.2389 - acc: 0.8956 - val_loss: 0.3438 - val_acc: 0.8598\n",
            "\n",
            "Epoch 36/50\n",
            " - 97s - loss: 0.2382 - acc: 0.8961 - val_loss: 0.3436 - val_acc: 0.8596\n",
            "\n",
            "Epoch 37/50\n",
            " - 94s - loss: 0.2368 - acc: 0.8965 - val_loss: 0.3476 - val_acc: 0.8612\n",
            "\n",
            "Epoch 38/50\n",
            " - 95s - loss: 0.2350 - acc: 0.8974 - val_loss: 0.3521 - val_acc: 0.8610\n",
            "\n",
            "Epoch 39/50\n",
            " - 95s - loss: 0.2346 - acc: 0.8976 - val_loss: 0.3469 - val_acc: 0.8597\n",
            "\n",
            "Epoch 40/50\n",
            " - 93s - loss: 0.2329 - acc: 0.8982 - val_loss: 0.3486 - val_acc: 0.8587\n",
            "\n",
            "Epoch 41/50\n",
            " - 92s - loss: 0.2315 - acc: 0.8987 - val_loss: 0.3602 - val_acc: 0.8610\n",
            "\n",
            "Epoch 42/50\n",
            " - 89s - loss: 0.2307 - acc: 0.8991 - val_loss: 0.3550 - val_acc: 0.8585\n",
            "\n",
            "Epoch 43/50\n",
            " - 92s - loss: 0.2295 - acc: 0.8994 - val_loss: 0.3581 - val_acc: 0.8568\n",
            "\n",
            "Epoch 44/50\n",
            " - 92s - loss: 0.2284 - acc: 0.8999 - val_loss: 0.3603 - val_acc: 0.8602\n",
            "\n",
            "Epoch 45/50\n",
            " - 90s - loss: 0.2272 - acc: 0.9004 - val_loss: 0.3607 - val_acc: 0.8590\n",
            "\n",
            "Epoch 46/50\n",
            " - 91s - loss: 0.2263 - acc: 0.9007 - val_loss: 0.3729 - val_acc: 0.8599\n",
            "\n",
            "Epoch 47/50\n",
            " - 91s - loss: 0.2246 - acc: 0.9015 - val_loss: 0.3588 - val_acc: 0.8573\n",
            "\n",
            "Epoch 48/50\n",
            " - 90s - loss: 0.2247 - acc: 0.9016 - val_loss: 0.3646 - val_acc: 0.8576\n",
            "\n",
            "Epoch 49/50\n",
            " - 92s - loss: 0.2237 - acc: 0.9021 - val_loss: 0.3682 - val_acc: 0.8577\n",
            "\n",
            "Epoch 50/50\n",
            " - 91s - loss: 0.2226 - acc: 0.9028 - val_loss: 0.3681 - val_acc: 0.8571\n",
            "\n",
            "Lowest Validation Loss:\n",
            "0.31709421374638874\n",
            "100%|██████████| 5/5 [7:07:11<00:00, 5126.29s/it, best loss: -0.6354889180819193]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubZdE_ITPvjQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8125e132-8e0a-4586-883f-0c2d2f844138"
      },
      "source": [
        "import pickle\n",
        "with open('X_test_scaled.pickle', 'rb') as handle:\n",
        "    x_test = pickle.load(handle)\n",
        "with open('y_test_categorical.pickle', 'rb') as handle:\n",
        "    y_test = pickle.load(handle)\n",
        "\n",
        "print(\"Evalutation of best performing model:\")\n",
        "print(best_model.evaluate(x_test,y_test))\n",
        "print(\"Best performing model chosen hyper-parameters:\")\n",
        "print(best_run)\n",
        "\n",
        "best_model.save('Hyperas_tuned_DNN.h5')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evalutation of best performing model:\n",
            "108000/108000 [==============================] - 2s 19us/step\n",
            "[0.6724774932861328, 0.6613055555555556]\n",
            "Best performing model chosen hyper-parameters:\n",
            "{'Dense': 0, 'Dense_1': 2, 'Dropout': 0.9758185183456943, 'Dropout_1': 0.288662535902546, 'batch_size': 0, 'epochs': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70wNCZuiHhsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('Hyperas_tuned_DNN.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEnpIGg1812u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf55d5eb-e229-4806-a120-72e93aaf90bd"
      },
      "source": [
        "model_loss, model_accuracy = best_model.evaluate(\n",
        "    X_test_scaled, y_test_categorical, verbose=2)\n",
        "print(\n",
        "    f\"Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural Network - Loss: 0.6724774932861328, Accuracy: 0.6613055555555556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BnMNqXT-esS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIyW90bzYdyW",
        "colab_type": "text"
      },
      "source": [
        "## Untuned Keras w LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulzyzgj6Yf-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def data(proportion=1.0):\n",
        "  '''Returns a proportion of the rows from the csv files in a scaled train/test split'''\n",
        "  nlp = en_core_web_lg.load()\n",
        "\n",
        "  train_df = pd.read_csv(\"https://mt-proj-001.s3.us-east-2.amazonaws.com/train.csv\", \n",
        "                         header=None, nrows=math.floor(3600000 * proportion))\n",
        "  train_df = train_df.rename(columns={0:'label',2:'review'}).drop(columns=[1])\n",
        "\n",
        "\n",
        "  test_df = pd.read_csv(\"https://mt-proj-001.s3.us-east-2.amazonaws.com/test.csv\", \n",
        "                        header=None, nrows=math.floor(400000 * proportion))\n",
        "  test_df = test_df.rename(columns={0:'label',2:'review'}).drop(columns=[1])\n",
        "\n",
        "\n",
        "  #pull out the review labelling into a Series\n",
        "  test_labels = test_df['label'].replace(1,0).replace(2,1)\n",
        "  \n",
        "  train_labels = train_df['label'].replace(1,0).replace(2,1)\n",
        "\n",
        "  print(\"Beginning tokenizing...\")\n",
        "  #create a generator for the list of vectors for each review text\n",
        "  train_tokens = []\n",
        "  for doc in nlp.pipe(train_df['review'], disable=[\"tagger\",\"parser\",\"ner\"]):\n",
        "    token_vecs = []\n",
        "    for token in doc:\n",
        "      token_vecs.append(token.vector)\n",
        "    train_tokens.append(token_vecs) \n",
        "\n",
        "  test_tokens = []\n",
        "  for doc in nlp.pipe(test_df['review'], disable=[\"tagger\",\"parser\",\"ner\"]):\n",
        "    token_vecs = []\n",
        "    for token in doc:\n",
        "      token_vecs.append(token.vector)\n",
        "    test_tokens.append(token_vecs)  \n",
        "\n",
        "  print(\"Padding and truncating...\")\n",
        "  #for each set of vectors, find the median length, and truncate/pad each set to the median length\n",
        "  #yes this is very slow\n",
        "  PAD_VECTOR = [0.0] * 300\n",
        "  length_list = []\n",
        "  for vector_list in train_tokens:\n",
        "    length_list.append(len(vector_list))\n",
        "  for vector_list in test_tokens:\n",
        "    length_list.append(len(vector_list))\n",
        "\n",
        "  median_len = int(np.median(length_list))\n",
        "  print(f\"Median length of a review is: {median_len}\")\n",
        "\n",
        "  for i in range(len(train_tokens)):\n",
        "    if len(train_tokens[i]) > median_len:\n",
        "      train_tokens[i] = train_tokens[i][:median_len]\n",
        "    elif len(train_tokens[i]) < median_len:\n",
        "      train_tokens[i] += (median_len - len(train_tokens[i])) * [PAD_VECTOR]\n",
        "    if len(train_tokens[i]) != median_len:\n",
        "      print('oops', i)\n",
        "  for i in range(len(test_tokens)):\n",
        "    if len(test_tokens[i]) > median_len:\n",
        "      test_tokens[i] = test_tokens[i][:median_len]\n",
        "    elif len(test_tokens[i]) < median_len:\n",
        "      test_tokens[i] += (median_len - len(test_tokens[i])) * [PAD_VECTOR]\n",
        "\n",
        "  #debug prints\n",
        "  print(len(train_tokens[0]))\n",
        "  print(len(train_tokens[0][0]))\n",
        "  print(type(train_tokens))\n",
        "  print(type(train_tokens[0]))\n",
        "  print(type(train_tokens[0][0]))\n",
        "\n",
        "  #variable reassignment\n",
        "  X_train = np.array(train_tokens)\n",
        "  X_test = np.array(test_tokens)\n",
        "  y_train = train_labels\n",
        "  y_test = test_labels\n",
        "\n",
        "  print(f\"Train token shape: {np.shape(X_train)} \\n Train label shape: {np.shape(y_train)}\")\n",
        "  print(\"Scaling, about to return...\")\n",
        "  # Create a StandardScater model and fit it to the training data\n",
        "  # reshaping due to the data being 3D\n",
        "  scaler = StandardScaler()\n",
        "  X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "  X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
        "  #X_scaler = StandardScaler().fit(X_train)\n",
        "\n",
        "  #X_train_scaled = X_scaler.transform(X_train)\n",
        "  #X_test_scaled = X_scaler.transform(X_test)\n",
        "\n",
        "\n",
        "  y_train_categorical = to_categorical(y_train)\n",
        "  y_test_categorical = to_categorical(y_test)\n",
        "\n",
        "  return X_train_scaled, y_train_categorical, X_test_scaled, y_test_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0vBs703fZEv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0552c197-3c6b-4a90-b823-ebe0d2efd810"
      },
      "source": [
        "PROPORTION = 0.007\n",
        "X_train_scaled, y_train_categorical, X_test_scaled, y_test_categorical = data(PROPORTION)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning tokenizing...\n",
            "Padding and truncating...\n",
            "Median length of a review is: 77\n",
            "77\n",
            "300\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'numpy.ndarray'>\n",
            "Train token shape: (25200, 77, 300) \n",
            " Train label shape: (25200,)\n",
            "Scaling, about to return...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C2kQEEzsX88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "484f2abd-4d78-47a6-cbf8-d4bc1e5011a9"
      },
      "source": [
        "print(X_train_scaled.shape, X_test_scaled.shape, y_train_categorical.shape, y_test_categorical.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25200, 77, 300) (2800, 77, 300) (25200, 2) (2800, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sYZP7wgZJ6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "be5e174d-fa1a-4ebb-b587-bcb145f1c792"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "\n",
        "neurons = 20\n",
        "#input_dim = X_train_scaled.shape[1]\n",
        "batch_size = 100\n",
        "time_step = 77\n",
        "data_dim = 300\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(Dense(units=neurons, activation='relu', input_dim=input_dim))\n",
        "model.add(LSTM(neurons, dropout=0.1, recurrent_dropout=0.1, stateful=True, return_sequences=True, batch_input_shape=(batch_size, time_step, data_dim)))\n",
        "model.add(LSTM(neurons, dropout=0.1, recurrent_dropout=0.1, stateful=True))\n",
        "model.add(Dense(units=2, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_2 (LSTM)                (100, 77, 20)             25680     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (100, 20)                 3280      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (100, 2)                  42        \n",
            "=================================================================\n",
            "Total params: 29,002\n",
            "Trainable params: 29,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l_PJcJCZU5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ApAmtHXtniX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "42614626-0442-49f4-8ae1-8778c1f7eae1"
      },
      "source": [
        "model.fit(\n",
        "    X_train_scaled,\n",
        "    y_train_categorical,\n",
        "    epochs=100,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_split =0.15\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 21420 samples, validate on 3780 samples\n",
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}